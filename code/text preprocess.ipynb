{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f89725b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import jieba\n",
    "import hanlp\n",
    "from datetime import datetime\n",
    "#从自写工具包中导入：修饰词判定、修饰词查找\n",
    "#from utils import modifier_judge, modifier_search, modifier_search_for_mode2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e898b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                   \r"
     ]
    }
   ],
   "source": [
    "Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_BASE_ZH)\n",
    "Dep = hanlp.load(hanlp.pretrained.dep.CTB9_DEP_ELECTRA_SMALL)\n",
    "TOK = hanlp.load( hanlp.pretrained.tok.CTB9_TOK_ELECTRA_BASE)\n",
    "POS = hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1982f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "氛围\n",
      "推动城市化\n",
      "111\n",
      "推动城市化\n"
     ]
    }
   ],
   "source": [
    "#“就业再就业”变成“就业和再就业”\n",
    "#\"抓住....的机遇\"要删去；“是把”开头变成“把”；“数字开头要注意”\n",
    "\n",
    "\n",
    "\n",
    "def get_stopwords_list():\n",
    "    stopwords = [line.strip() for line in open('../hit_stopwords.txt',encoding='UTF-8').readlines()]\n",
    "    return stopwords\n",
    "\n",
    "\n",
    "\n",
    "stopwords = get_stopwords_list()\n",
    "#print(get_stopwords_list())\n",
    "\n",
    "wrong_list = [\"答卷\",\"成绩单\", \"底色\", \"胡锦涛\", \"叫响\", \"蓝图\", \"排头兵\",\"习近平\",\"新篇章\", \"金字招牌\",\"局面\", \"之美\",\n",
    "              \"让人民满意\",\"难忘\", \"一盘棋\", \"一步一个脚印\", \"一年干\", \"总书记\", \"党中央\", \"党委\", \"感情\", \n",
    "              \"勇于\", \"姿态\",\"生根\", \"真脱贫\", \"脱真贫\",\"量力而行\", \"声音\", \"大旗\", \"尽力而为\", \"决胜\", \"新境界\",\n",
    "              \"精神境界\", \"紧日子\", \"全心全意\", \"心上\", \"主旋律\", \"压舱石\", \"中国故事\", \"谱写\", \"书写\", \"文章\",\"民生之\", \n",
    "              \"久久为功\", \"组合拳\", \"攻坚克难\", \"四梁八柱\", \"脚踏实地\", \"决胜\", \"保卫战\", \"打攻坚战\", \"之花\", \"蛋糕\", \"氛围\", \n",
    "              \"轨道\",  \"高潮\", \"敞开\", \"保驾护航\", \"以人民为中心\", \"为代价\", \"店小二\"\n",
    "              ]  \n",
    "              #\"脚印\", \"凝聚\", \"共识\", \"汇集\", \"智慧\", \"口袋\", \"坚定不移\", \"护航\", \"沃土\", \"先锋\", \"桥头堡\", \"领头雁\",\n",
    "\n",
    "\n",
    "def find_matching_element(lst, s):\n",
    "    for item in lst:\n",
    "        if item in s:\n",
    "            return item\n",
    "    return None\n",
    "\n",
    "#if not find_matching_element(wrong_list, \"努力形成全社会关心困难和下岗职业生活的氛围\"):\n",
    "print(find_matching_element(wrong_list, \"努力形成全社会关心困难和下岗职业生活的氛围\"))\n",
    "\n",
    "# 中文数字到整数的映射\n",
    "chinese_numbers = ['一', '二', '三', '四', '五', '六', '七', '八', '九','十','是','要','我','们','&', '、', '（', '）',\n",
    "                   ')', '(']\n",
    "chinese_numbers_true = ['一', '二', '三', '四', '五', '六', '七', '八', '九','十']\n",
    "measure_word_list = [\"批\", \"产\", \"张\"]\n",
    "\n",
    "#删去没有用的词，考虑“使？”\n",
    "strings_to_remove = [\"继续\", \"切实\", \"不断\",\"必须\" ,\"一如既往地\", \"一如既往\", \"花大力气\", \"下大力气\", \"下大气力\", \"进一步\",\n",
    "                     \"驰而不息地\", \"以更大决心和勇气\", \"集中力量\", \"以更大力度\", \"以更大的力度\", \"三大战役\",\n",
    "                     \"全力以赴\", \"充满感情、全力以赴地去解决\", \"千方百计\",\"下大力\", \"大胆\", \"聚焦聚力\", \"加大力度\",\n",
    "                     \"举全区之力\", \"举全省之力\", \"举全市之力\", \"持之以恒地\", \"歼灭战\", \"大会战\", \"人民战争\", \"齐心协力\",\n",
    "                     \"集中兵力\", \"攻坚战持久战\", \"集中精力\", \"重拳\", \"这场硬仗\", \"五场战役\", \"重点战役\", \"两个攻坚战\", \n",
    "                     \"积极创造条件\", \"一定要\", '毫不动摇地', '毫不松懈地', \"实现新跨越\", \"实现新的跨越\", \n",
    "                     \"迈出实质性步伐\", \"迈出重大步伐\", \"迈出新步伐\", \"迈出更大步伐\", \"迈出实质性步伐\", \"迈出重要步伐\",\n",
    "                     \"迈出大的步伐\", \"迈出较大步伐\", '迈出新的步伐', \"迈出更大的步伐\", \"迈出坚实步伐\",\n",
    "                     \"取得新进展\", \"取得重大进展\", \"取得突破性进展\", \"取得新的进展\", \"取得重要进展\", \"取得实质性进展\",\n",
    "                     \"取得更大进展\", \"取得较大进展\", \"取得明显进展\", \"取得新的更大进展\",\n",
    "                     \"取得新突破\", \"取得重大突破\", \"取得新的突破\", \"取得实质性突破\",\n",
    "                     \"实现更大作为\", \"展现更大作为\"]\n",
    "\n",
    "\n",
    "patterns_leading = [\"重点要\", \"重点是\", \"尤其要\", \"尤其是\", \"特别是\", \"特别要\", \"重中之重是\", \"重要的是\", \"重点在于\", \"关键在于\", \n",
    "                    \"毫不动摇\", \"毫不松懈\", \"驰而不息\", \"随着\", \"下功夫\",\"以绣花功夫\",\"一定要\", \"想方设法\", \"理直气壮\", \n",
    "                    \"下决心\", \"更大力度\", \"持之以恒\", \"创造条件\", \"积极\", \"逐步\", \n",
    "                    \"1、\", \"2、\", \"3、\", \"4、\", \"5、\", \"6、\", \"7、\", \"8、\", \"9、\"]\n",
    "\n",
    "patterns_ending = [\"力度\", \"持久战\", \"、\", \"会战\", \"硬仗\", \"战役\", \"等\", \"四大\",\"五大\", \"三大\", \"十大\", \"攻坚战\", \"进程\",\n",
    "                   \"步伐\", \"的\", \"新跨越\", \"建设\"]###\"建设\"!!!!!!!!!!!!!!\n",
    "\n",
    "Rang_leading = [\"绝不\", \"真正\", \"努力\", \"不\"]\n",
    "\n",
    "#让、努力让\n",
    "\n",
    "def remove_strings(text, strings_to_remove):\n",
    "    # 将字符串列表转换为正则表达式模式\n",
    "    pattern = '|'.join(re.escape(s) for s in strings_to_remove)\n",
    "    # 使用正则表达式模式进行匹配并删除\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_leading_matches(string, patterns):\n",
    "    # 将模式组合成正则表达式\n",
    "    pattern = '|'.join(re.escape(p) for p in patterns)\n",
    "    # 使用正则表达式匹配开头，并将其替换为空字符串\n",
    "    return re.sub('^(' + pattern + ')', '', string)\n",
    "\n",
    "def remove_ending_matches(string, patterns):\n",
    "    # 将模式组合成正则表达式\n",
    "    pattern = '|'.join(re.escape(p) for p in patterns)\n",
    "    # 使用正则表达式匹配结尾，并将其替换为空字符串patterns_ending\n",
    "    return re.sub('(' + pattern + ')$', '', string)\n",
    "\n",
    "\n",
    "def extract_substrings(text, start, end):\n",
    "    pattern = rf'{start}(.*?){end}'\n",
    "    matches = re.findall(pattern, text)\n",
    "    return matches[0]\n",
    "\n",
    "def move_stopwords(sentence_list, stopwords_list):\n",
    "    # 去停用词\n",
    "    out_list = []\n",
    "    for word in sentence_list:\n",
    "        if word not in stopwords_list:\n",
    "            if word != '\\t':\n",
    "                out_list.append(word)\n",
    "    return out_list\n",
    "\n",
    "\n",
    "def RangClean(l):\n",
    "    test_l = remove_leading_matches(l, Rang_leading)\n",
    "    if test_l.startswith('让'):\n",
    "        return ''\n",
    "    else:\n",
    "        return l\n",
    "    \n",
    "def yiClean(l):\n",
    "    de_judge = False\n",
    "    if l.startswith(\"坚持以\"):\n",
    "        l = l.replace('坚持以', '以')\n",
    "    if l.startswith(\"始终坚持以\"):\n",
    "        l = l.replace('始终坚持以', '以')\n",
    "    if l.startswith(\"以\"):\n",
    "        if any(l.endswith(substr) for substr in [\"为主\"]):\n",
    "            l = '推动'+l[1:-2]\n",
    "            de_judge = True\n",
    "        if any(l.endswith(substr) for substr in [\"为重点\", \"为载体\", \"为基础\", \"为目标\", \"为依托\", \"为龙头\", \"为纽带\",\n",
    "                                                 \"为动力\", \"为契机\", \"为指导\", \"为牵引\", \"为支撑\", \"为平台\", \"为统领\",\n",
    "                                                 \"为主体\", \"为导向\", \"为引领\", \"为方向\", \"为根本\", \"为抓手\", \"为支点\",\n",
    "                                                 \"为杠杆\", \"为主题\", \"为路径\", \"为牵动\", \"为带动\", \"为中心\", \"为目的\",\n",
    "                                                 \"为主线\"]):\n",
    "            l = '推动'+l[1:-3]\n",
    "            de_judge = True\n",
    "        if any(l.endswith(substr) for substr in [\"为突破口\", \"为着力点\", \"为落脚点\", \"为切入点\", \"为总抓手\", \"为总目标\"]):\n",
    "            l = '推动'+l[1:-4]\n",
    "            de_judge = True\n",
    "        if any(l.endswith(substr) for substr in [\"为主要模式\", \"为战略基点\", \"为中心环节\", \"为重要抓手\", \"为主攻方向\",\n",
    "                                                 \"为战略指引\", \"为关键抓手\", \"为战略重点\", \"为主要措施\", \"为重要载体\"]):\n",
    "            l = '推动'+l[1:-5]\n",
    "            de_judge = True\n",
    "    if de_judge and len(l) < 5:\n",
    "        return ''\n",
    "    return l\n",
    "    \n",
    "def zaiClean(l):\n",
    "    if '在' in l:\n",
    "        l = l.replace('努力在', '在')\n",
    "        l = l.replace('力争在', '在')\n",
    "        l = l.replace('力求在', '在')\n",
    "        l = l.replace('务必在', '在')\n",
    "        if l.endswith(\"前提下\"):\n",
    "            l = l[1:-3]\n",
    "            l = l.rstrip('的')\n",
    "        if l.endswith(\"基础上\"):\n",
    "            l = l[1:-3]\n",
    "            l = l.rstrip('的')\n",
    "        if l.endswith(\"上下功夫\"):\n",
    "            l = '推动'+l[1:-4]\n",
    "        if l.endswith(\"方面\"):\n",
    "            l = '推动'+l[1:-2]\n",
    "        if l.endswith(\"上\"):\n",
    "            l = '推动'+l[1:-1]\n",
    "        if \"上有新\" in l:\n",
    "            l = l.split(\"上有新\")[0]\n",
    "            l = '推动' + l[1:]\n",
    "    if l.endswith('方面'):\n",
    "        l = l[1:-2]\n",
    "    return l\n",
    "    \n",
    "    #if \"上\" in l:\n",
    "\n",
    "    \n",
    "def basicClean(l):\n",
    "    l = l.replace('\\n', '')\n",
    "    l = l.replace('—', '')\n",
    "    l = l.replace('•', '')\n",
    "    l = l.replace('‰', '%')\n",
    "    l = l.replace('%左右', '%')\n",
    "    l = l.replace('%以内', '%')\n",
    "    l = l.replace('牛鼻子', '重点')\n",
    "    l = l.replace('主战场', '重点')\n",
    "    l = l.replace('有机结合', '结合')\n",
    "    \n",
    "    patternH = r'\\[(\\d+)\\]'\n",
    "    l = re.sub(patternH, '', l)\n",
    "    l = remove_strings(l, strings_to_remove)\n",
    "    \n",
    "    if l.startswith('在') and l.endswith(\"的同时\"):\n",
    "        l = l.lstrip('在')\n",
    "        l = l[:-3]\n",
    "        \n",
    "    \n",
    "    if '路子' in l:\n",
    "        l = l.replace('路子', '道路')\n",
    "\n",
    "    pattern = r\"像.*?一样\"\n",
    "    pattern2 = r\"大兴.*之风\"\n",
    "    matches = re.findall(pattern, l)\n",
    "    for match in matches:\n",
    "        #print(match)\n",
    "        l = l.replace(match, '')\n",
    "    \n",
    "    matches2 = re.findall(pattern2, l)\n",
    "    for match2 in matches2:\n",
    "        l = l.replace(match2, '')\n",
    "    \n",
    "    if l:\n",
    "        while l[0] in chinese_numbers and len(l)>2:\n",
    "            if l[0] in chinese_numbers_true:\n",
    "                if l[1] not in measure_word_list:\n",
    "                    l = l.lstrip(l[0])\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                l = l.lstrip(l[0])\n",
    "\n",
    "                \n",
    "        l = remove_leading_matches(l, patterns_leading)\n",
    "        l = remove_leading_matches(l, patterns_leading)\n",
    "        if l.startswith('为') and l.endswith(\"创造条件\"):\n",
    "            l = l.lstrip('为')\n",
    "            l = l[:-4]\n",
    "        \n",
    "        if l.startswith('加快') and l.endswith(\"步伐\"):\n",
    "            l = l[:-2]\n",
    "            l = l.lstrip('的')\n",
    "        \n",
    "        while l != remove_ending_matches(l, patterns_ending):   \n",
    "            l = remove_ending_matches(l, patterns_ending)\n",
    "        #l = remove_ending_matches(l, patterns_ending)\n",
    "        if '腾笼换鸟' in l:\n",
    "            l = l.replace( '腾笼换鸟', '转型升级')\n",
    "            if '、凤凰涅槃' in l:\n",
    "                l = l.replace( '凤凰涅槃', '')\n",
    "        #    l = extract_substrings(l, '打好', '攻坚战')\n",
    "        #    l = '推进'+l\n",
    "        #    l = l.replace('两个', '')\n",
    "        #    l = l.replace('三个', '')\n",
    "        return l\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "ll = '推动城市化建设'\n",
    "if basicClean(ll):\n",
    "    print(basicClean(ll))\n",
    "    print('111')\n",
    "    fix_ll = RangClean(basicClean(ll))\n",
    "    print(fix_ll)\n",
    "    #print(fix_ll[-3:])\n",
    "    if '在' in fix_ll:\n",
    "        fix_ll = zaiClean(fix_ll)\n",
    "        print(fix_ll)\n",
    "        #print(fix_ll[-3:])\n",
    "    if '以' in fix_ll:\n",
    "        fix_ll = yiClean(fix_ll)\n",
    "        print(fix_ll)\n",
    "    \n",
    "        \n",
    "        \n",
    "    #print(zaiClean(basicClean(ll)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "463d58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_all_elements(list_a, list_b):\n",
    "    return set(list_b).issubset(set(list_a))\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    return [x for i, x in enumerate(lst) if x not in lst[:i]]\n",
    "\n",
    "def modifier_judge(index, dependency):\n",
    "    modifyJudge = False\n",
    "    for i1 in dependency:\n",
    "        if i1[1] in ['nn', 'prep', 'nummod', 'amod','mmod', 'assmod','vmod',  'advmod', 'dep', 'clf','nsubj','rcmod'] and i1[0]-1 == index:\n",
    "            modifyJudge = True\n",
    "        if i1[1] == 'range' and i1[0]-1 == dependency[index][0]-1:\n",
    "            modifyJudge = True    \n",
    "    return modifyJudge\n",
    "\n",
    "def modifier_judge_conj(index, dependency):\n",
    "    modifyJudge = False\n",
    "    for i1 in dependency:\n",
    "        if i1[1] in ['nn', 'prep', 'nummod', 'amod','mmod', 'assmod','vmod', 'advmod', 'dep', 'clf','nsubj', 'dobj', 'rcmod'] and i1[0]-1 == index:\n",
    "            modifyJudge = True\n",
    "        if i1[1] == 'range' and i1[0]-1 == dependency[index][0]-1:\n",
    "            modifyJudge = True    \n",
    "    return modifyJudge\n",
    "\n",
    "\n",
    "def modifier_search(index, seg_list, dependency):\n",
    "    phrase=[]\n",
    "    phrase.append(seg_list[dependency[index][0]-1])\n",
    "    #multiple_key = False\n",
    "    #key.append(seg_list[index])\n",
    "    #for index1, i1 in enumerate(dependency):\n",
    "    #    if i1[1] in ['conj'] and seg_list[i1[0]-1] == seg_list[index]: \n",
    "    #        key.append(seg_list[index1])\n",
    "    phrase.append(seg_list[index])\n",
    "    for index2, i2 in enumerate(dependency):\n",
    "        if i2[1] in ['nn', 'prep', 'nummod', 'amod', 'assmod','vmod', 'advmod', 'dep','mmod', 'clf','rcmod'] and i2[0]-1 == index:\n",
    "            if seg_list[index2] not in phrase:\n",
    "                phrase.append(seg_list[index2])\n",
    "            for index2_1, i2_1 in enumerate(dependency):\n",
    "                if i2_1[1] in ['nn', 'nummod', 'amod', 'assmod','vmod', 'dep', 'advmod', 'clf','rcomp'] and i2_1[0]-1 == index2:\n",
    "                    if seg_list[index2_1] not in phrase:\n",
    "                        phrase.append(seg_list[index2_1])\n",
    "                    for index2_2, i2_2 in enumerate(dependency):\n",
    "                        if i2_2[1] in ['nn', 'nummod', 'amod', 'assmod','vmod', 'advmod','rcomp'] and i2_2[0]-1 == index2_1:\n",
    "                            if seg_list[index2_2] not in phrase:\n",
    "                                phrase.append(seg_list[index2_2])\n",
    "                            for index2_4, i2_4 in enumerate(dependency):\n",
    "                                if i2_4[1] in ['nn', 'nummod', 'amod', 'assmod','vmod', 'advmod','rcomp'] and i2_4[0]-1 == index2_2:\n",
    "                                    if seg_list[index2_4] not in phrase:\n",
    "                                        phrase.append(seg_list[index2_4])\n",
    "#                                    for index2_5, i2_5 in enumerate(dependency):\n",
    "#                                        if i2_5[1] in ['nn', 'prep', 'nummod', 'amod', 'assmod','vmod', 'advmod', 'clf','nsubj'] and i2_5[0]-1 == index2_4:\n",
    "#                                            phrase.append(seg_list[index2_5])                            \n",
    "        if i2[1] == 'range' and i2[0]-1 == dependency[index][0]-1:\n",
    "            for index2_3, i2_3 in enumerate(dependency):\n",
    "                if i2_3[1] == 'nummod' and i2_3[0]-1 == index2_3:\n",
    "                    if seg_list[index2] not in phrase and seg_list[index2_3] not in phrase:\n",
    "                        phrase.append(seg_list[index2])\n",
    "                        phrase.append(seg_list[index2_3])\n",
    "\n",
    "                \n",
    "    return phrase\n",
    "\n",
    "def modifier_search_for_mode2(index, seg_list, dependency):\n",
    "    phrase=[]\n",
    "    #multiple_key = False\n",
    "    #key.append(seg_list[index])\n",
    "    #for index1, i1 in enumerate(dependency):\n",
    "    #    if i1[1] in ['conj'] and seg_list[i1[0]-1] == seg_list[index]: \n",
    "    #        key.append(seg_list[index1])\n",
    "    phrase.append(seg_list[index])\n",
    "    for index2, i2 in enumerate(dependency):\n",
    "        if i2[1] in ['nn', 'prep', 'nummod', 'amod', 'assmod','vmod', 'range', 'mmod','advmod','rcmod', 'dep'] and i2[0]-1 == index:\n",
    "            if seg_list[index2] not in phrase:\n",
    "                phrase.append(seg_list[index2])\n",
    "            for index2_1, i2_1 in enumerate(dependency):\n",
    "                if i2_1[1] in ['nn', 'prep', 'nummod', 'amod', 'assmod','vmod', 'advmod','dep'] and i2_1[0]-1 == index2:\n",
    "                    if seg_list[index2_1] not in phrase:\n",
    "                        phrase.append(seg_list[index2_1])\n",
    "    return phrase\n",
    "\n",
    "def modifier_search_for_mode2_conj(index, seg_list, dependency):\n",
    "    phrase=[]\n",
    "    #multiple_key = False\n",
    "    #key.append(seg_list[index])\n",
    "    #for index1, i1 in enumerate(dependency):\n",
    "    #    if i1[1] in ['conj'] and seg_list[i1[0]-1] == seg_list[index]: \n",
    "    #        key.append(seg_list[index1])\n",
    "    phrase.append(seg_list[index])\n",
    "    for index2, i2 in enumerate(dependency):\n",
    "        if i2[1] in ['nn', 'nsubj', 'dobj', 'prep', 'nummod', 'amod', 'assmod','vmod', 'range', 'mmod','advmod','rcmod', 'dep'] and i2[0]-1 == index:\n",
    "            if seg_list[index2] not in phrase:\n",
    "                phrase.append(seg_list[index2])\n",
    "            for index2_1, i2_1 in enumerate(dependency):\n",
    "                if i2_1[1] in ['nn', 'prep', 'nummod', 'amod', 'assmod','vmod', 'advmod','dep'] and i2_1[0]-1 == index2:\n",
    "                    if seg_list[index2_1] not in phrase:\n",
    "                        phrase.append(seg_list[index2_1])\n",
    "    return phrase\n",
    "\n",
    "def read_time_list(path):\n",
    "    list_time=[]\n",
    "    with open(path, 'r') as time_read:\n",
    "        for line in time_read:\n",
    "            times = line.split(',')\n",
    "            times.remove('\\n')\n",
    "            list_time.append(times)\n",
    "    return list_time\n",
    "\n",
    "\n",
    "def phrase2vec(lines):\n",
    "    phrases_list = []\n",
    "    for  phrases in lines:\n",
    "        if phrases[0] in vectors:\n",
    "            array = vectors[phrases[0]].reshape(1,300)\n",
    "        else:\n",
    "            print('word can not find')\n",
    "        for i in range(1,len(phrases)):\n",
    "            if phrases[i] in vectors:\n",
    "                array = np.append(array, vectors[phrases[i]].reshape(1,300),axis = 0)\n",
    "            else:\n",
    "                print('word can not find')\n",
    "        phrases_list.append(array)\n",
    "    return phrases_list\n",
    "\n",
    "def phrase_extraction(sentence, sentenceO):\n",
    "    seg_list = TOK([sentence][0])\n",
    "    seg_list = move_stopwords(seg_list, stopwords)\n",
    "    if seg_list:\n",
    "        position = POS([seg_list], conll=False)[0]\n",
    "        dependency = Dep([seg_list], conll=False)[0]\n",
    "        #print(seg_list)\n",
    "        #print(position)\n",
    "        #print(dependency)\n",
    "        list1 = []\n",
    "        list2 = []\n",
    "        keys = []\n",
    "    #list2 = \n",
    "        for index, i in enumerate(dependency):\n",
    "            #复合名词修饰、介词性修饰语、数词修饰语、形容词修饰语、关联修饰\n",
    "            #if i[1] in ['nn', 'prep', 'nummod', 'amod', 'assmod']:\n",
    "            #'ccomp',\n",
    "            if i[1] in ['dobj', 'pobj','lobj','nsubj'] or (i[1] == 'ccomp' and position[index] == 'NN'):\n",
    "                mode1 = False\n",
    "                if position[i[0]-1] in ['VV', 'VE', 'VC','P'] and dependency[i[0]-1][1] not in ['prep','nn', 'nummod', 'amod', 'assmod','vmod', 'advmod', 'rcmod']:\n",
    "                    if seg_list[index] not in keys:\n",
    "                        if modifier_judge(index, dependency) == True:\n",
    "                            #print(seg_list[index])\n",
    "                            keys.append(seg_list[index])\n",
    "                            list1.append(modifier_search(index, seg_list, dependency))\n",
    "                        else:\n",
    "                            if i[1] in ['dobj', 'pobj','lobj','nsubj']:\n",
    "                                keys.append(seg_list[index])\n",
    "                                list1.append([seg_list[i[0]-1],seg_list[index]])\n",
    "                else:\n",
    "                    if modifier_judge(index, dependency) == True:\n",
    "                        list2.append(modifier_search_for_mode2(index, seg_list, dependency))\n",
    "                    else:\n",
    "                        list2.append([seg_list[index]])\n",
    "        for index2, i2 in enumerate(dependency):        \n",
    "            if i2[1] =='conj':\n",
    "                if position[index2] not in ['VV', 'VE', 'VC','P']:\n",
    "                    if modifier_judge_conj(index2, dependency) == True:\n",
    "                        if modifier_search_for_mode2_conj(index2, seg_list, dependency) not in list2:\n",
    "                            list2.append(modifier_search_for_mode2_conj(index2, seg_list, dependency))\n",
    "                    else:\n",
    "                        if [seg_list[index2]] not in list2:\n",
    "                            list2.append([seg_list[index2]])\n",
    "                else:\n",
    "                    check_conj = True\n",
    "                    for check_list1 in list1:\n",
    "                        if seg_list[index2] in check_list1:\n",
    "                            check_conj = False\n",
    "                    if check_conj:\n",
    "                        if modifier_judge(index2, dependency) == True:\n",
    "                            if modifier_search_for_mode2(index2, seg_list, dependency) not in list2:\n",
    "                                list2.append(modifier_search_for_mode2(index2, seg_list, dependency))\n",
    "                        else:\n",
    "                            if [seg_list[index2]] not in list2:\n",
    "                                list2.append([seg_list[index2]])\n",
    "            if i2[1] == 'root' and position[index2] in ['NN', 'NT', 'NR','M']:\n",
    "                if modifier_judge(index, dependency) == True:\n",
    "                    list2.append(modifier_search_for_mode2(index, seg_list, dependency))\n",
    "            if i2[1] == 'range' and position[i2[0]-1] in ['VV', 'VE', 'VC','P']:\n",
    "                if modifier_judge(index2, dependency) == True:\n",
    "                    list2.append(modifier_search_for_mode2(index2, seg_list, dependency))\n",
    "            #if i2[1] == 'ccomp':\n",
    "            #    if modifier_judge(index2, dependency) == True:\n",
    "            #        list2.append(modifier_search_for_mode2(index2, seg_list, dependency))\n",
    "        for key2 in list2:\n",
    "            if key2[0] not in keys:\n",
    "                keys.append(key2[0])\n",
    "                \n",
    "        # 传统方式，使用列表推导式\n",
    "        #print(list1)\n",
    "        #print(list2)\n",
    "        keys = remove_duplicates(keys)\n",
    "        list2 = remove_duplicates(list2)\n",
    "        \n",
    "        for xOfList1 in list1:\n",
    "            for xOfList2 in list2:\n",
    "                if contains_all_elements(xOfList1[1:], xOfList2):\n",
    "                    list2.remove(xOfList2)\n",
    "\n",
    "        # 简写方式，使用 range\n",
    "        squares = [x**2 for x in range(10)]\n",
    "        return keys, list1, list2, sentence, sentenceO\n",
    "    else:\n",
    "        return ['']\n",
    "    \n",
    "    \n",
    "\n",
    "def textModification(path):\n",
    "    num_text = 0\n",
    "    len_raw = 0\n",
    "        \n",
    "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        the_line = False\n",
    "        the_time = True\n",
    "        lines1=[]\n",
    "        lines2=[]\n",
    "        for line in f:\n",
    "            #line = line.replace(' ','')\n",
    "            #line = line.replace('　','')\n",
    "            line=\"\".join(line.split())\n",
    "            ret = re.findall(r'(\\d{4})\\s*[\\./年-]\\s*(\\d{1,2})\\s*[\\./月-]\\s*(\\d{1,2})\\s*日?', line)\n",
    "            #print(line)\n",
    "            if ret and the_time:\n",
    "                year, month, day = ret[0]\n",
    "                #time = '{}/{}/{}'.format(year, month.lstrip(), day.lstrip())\n",
    "                time = datetime(int(year), int(month.lstrip()), int(day.lstrip()),hour=0,minute=0,second=0,microsecond=0)\n",
    "                time = time.strftime(\"%Y-%m-%d\")\n",
    "                the_time = False\n",
    "            if line.startswith('二&、'):\n",
    "                the_line = True\n",
    "           #     continue         \n",
    "            if the_line:\n",
    "                if line.startswith('\\n'):\n",
    "                    a=1\n",
    "                #elif line.count('，') >= 5:\n",
    "                    #lines.append(line)\n",
    "                    #print(line) \n",
    "                #    line = re.split('，|;|。', line)\n",
    "                 #   for l in line:\n",
    "                 #       if l != '\\n' and len(l) !=0:\n",
    "                 #           lines.append(l.replace('\\n', ''))\n",
    "                else:\n",
    "                    line = line.replace('％', '%')\n",
    "                    line = line.replace('%。左右', '%左右')\n",
    "                    line = line.replace('%。以内', '%以内')\n",
    "                    line = line.replace('“', '')\n",
    "                    line = line.replace('”', '')\n",
    "                    line = re.split('：|；|;|。|，|！|!|,|包括:', line)\n",
    "                    for l in line:\n",
    "                        if l != '\\n' and len(l) !=0:\n",
    "                            len_raw = len_raw+1\n",
    "                            if find_matching_element(wrong_list, l):\n",
    "                                continue\n",
    "                            else:\n",
    "                                l_c = basicClean(l)\n",
    "                                if l_c:\n",
    "                                    l_c = RangClean(basicClean(l_c))\n",
    "                                    if '在' in l_c:\n",
    "                                        l_c = zaiClean(l_c)\n",
    "                                    if '以' in l_c:\n",
    "                                        l_c = yiClean(l_c)\n",
    "                                    num_text = num_text+len(l_c)\n",
    "                                    phrases = phrase_extraction(l_c, l)\n",
    "                                    if phrases[0]:\n",
    "                                        lines2.append(phrases) \n",
    "                                    else:\n",
    "                                        cccccc=1\n",
    "                                    #print(l)\n",
    "    tuple1=tuple(lines2)\n",
    "    tuple1_summarize=tuple(lines1)\n",
    "    return tuple1, time, len_raw, num_text, tuple1_summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea2baea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['加强', '以', '全科', '医生', '为', '重点', '的', '基层', '医疗', '卫生', '队伍', '建设']\n",
      "[(0, 'root'), (5, 'prep'), (4, 'nn'), (2, 'pobj'), (12, 'rcmod'), (5, 'dobj'), (5, 'cpm'), (11, 'nn'), (11, 'nn'), (11, 'nn'), (12, 'nn'), (1, 'dobj')]\n",
      "['VV', 'P', 'NN', 'NN', 'VV', 'NN', 'DEC', 'NN', 'NN', 'NN', 'NN', 'NN']\n",
      "None\n",
      "(['队伍', '医生', '重点'], [['加强', '队伍', '为', '基层', '医疗', '卫生']], [['医生', '全科'], ['重点']], ' 加强以全科医生为重点的基层医疗卫生队伍', '更加奋发有为地推进发展')\n"
     ]
    }
   ],
   "source": [
    "T = TOK(['加强以全科医生为重点的基层医疗卫生队伍建设'][0])\n",
    "\n",
    "print(T)\n",
    "#print(111)\n",
    "\n",
    "\n",
    "\n",
    "#doc = HanLP(['新增可再生能源和原料用能不纳入能源消费总量控制'], tasks=['pos'])\n",
    "\n",
    "ccc = POS([T], conll=False)[0]\n",
    "#print(phrase_extraction('促进三大产业协调发展'))银行决定先取得信用评级\n",
    "dddd = Dep([T], conll=False)[0]\n",
    "print(dddd)\n",
    "print(ccc)\n",
    "\n",
    "\n",
    "l = ' 加强以全科医生为重点的基层医疗卫生队伍建设'\n",
    "print(find_matching_element(wrong_list, l))\n",
    "l = basicClean(l)\n",
    "l = zaiClean(l)\n",
    "l = yiClean(l)\n",
    "print(phrase_extraction(l, '更加奋发有为地推进发展'))\n",
    "\n",
    "#已有动词不能再当关键词，考虑删去更小的集合\n",
    "#print(phrase_extraction(l, '更加奋发有为地推进发展'))\n",
    "\n",
    "\n",
    "#以“五个一批”企业和“八个一百”工程为重点, 实现“二次创新”\n",
    "##和-以及：推进旅游、现代物流、金融保险以及社区服务等产业的发展；以“五个一批”企业和“八个一百”工程为重点。以“五个一批”企业和“八个一百”工程为重点\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd4966bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['安徽省', '山西省', '福建省', '上海市', '陕西省', '广西壮族自治区', '宁夏回族自治区', '重庆市', '浙江省', '贵州省', '湖北省', '湖南省', '青海省', '黑龙江省', '河北省', '河南省', '江西省', '天津市', '国务院', '辽宁省', '江苏省', '新疆维吾尔自治区', '山东省', '甘肃省', '四川省', '海南省', '内蒙古自治区', '北京市', '云南省', '西藏自治区', '吉林省', '广东省']\n"
     ]
    }
   ],
   "source": [
    "file_name_list = os.listdir('../new/policies_cleaned&undivided/reports_clean_0520_withoutG')\n",
    "file_name_list.remove('.DS_Store')\n",
    "#file_name_list.remove('国务院')\n",
    "print(file_name_list)\n",
    "\n",
    "#tatal_wrong_list = [\"试点\"]  \n",
    "#\"要\" “为”在开头,每段最后一句或者第一句的引号,\"以\"开头; 在。。。上取得重大突破\n",
    "\n",
    "#wrong_dict = {j:[] for j in tatal_wrong_list}\n",
    "#print(wrong_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "62c81b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "云南省  1998\n",
      "西藏自治区  1998\n",
      "111\n",
      "吉林省  1998\n",
      "广东省  1998\n",
      "云南省  1999\n",
      "西藏自治区  1999\n",
      "111\n",
      "吉林省  1999\n",
      "广东省  1999\n",
      "云南省  2000\n",
      "西藏自治区  2000\n",
      "111\n",
      "吉林省  2000\n",
      "广东省  2000\n",
      "云南省  2001\n",
      "西藏自治区  2001\n",
      "111\n",
      "吉林省  2001\n",
      "广东省  2001\n",
      "云南省  2002\n",
      "西藏自治区  2002\n",
      "吉林省  2002\n",
      "广东省  2002\n",
      "云南省  2003\n",
      "西藏自治区  2003\n",
      "吉林省  2003\n",
      "广东省  2003\n",
      "云南省  2004\n",
      "西藏自治区  2004\n",
      "吉林省  2004\n",
      "广东省  2004\n",
      "云南省  2005\n",
      "西藏自治区  2005\n",
      "吉林省  2005\n",
      "广东省  2005\n",
      "云南省  2006\n",
      "西藏自治区  2006\n",
      "吉林省  2006\n",
      "广东省  2006\n",
      "云南省  2007\n",
      "西藏自治区  2007\n",
      "吉林省  2007\n",
      "广东省  2007\n",
      "云南省  2008\n",
      "西藏自治区  2008\n",
      "吉林省  2008\n",
      "广东省  2008\n",
      "云南省  2009\n",
      "西藏自治区  2009\n",
      "吉林省  2009\n",
      "广东省  2009\n",
      "云南省  2010\n",
      "西藏自治区  2010\n",
      "吉林省  2010\n",
      "广东省  2010\n",
      "云南省  2011\n",
      "西藏自治区  2011\n",
      "吉林省  2011\n",
      "广东省  2011\n",
      "云南省  2012\n",
      "西藏自治区  2012\n",
      "吉林省  2012\n",
      "广东省  2012\n",
      "云南省  2013\n",
      "西藏自治区  2013\n",
      "吉林省  2013\n",
      "广东省  2013\n",
      "云南省  2014\n",
      "西藏自治区  2014\n",
      "吉林省  2014\n",
      "广东省  2014\n",
      "云南省  2015\n",
      "西藏自治区  2015\n",
      "吉林省  2015\n",
      "广东省  2015\n",
      "云南省  2016\n",
      "西藏自治区  2016\n",
      "吉林省  2016\n",
      "广东省  2016\n",
      "云南省  2017\n",
      "西藏自治区  2017\n",
      "吉林省  2017\n",
      "广东省  2017\n",
      "云南省  2018\n",
      "西藏自治区  2018\n",
      "吉林省  2018\n",
      "广东省  2018\n",
      "云南省  2019\n",
      "西藏自治区  2019\n",
      "吉林省  2019\n",
      "广东省  2019\n",
      "云南省  2020\n",
      "西藏自治区  2020\n",
      "吉林省  2020\n",
      "广东省  2020\n",
      "云南省  2021\n",
      "西藏自治区  2021\n",
      "吉林省  2021\n",
      "广东省  2021\n",
      "云南省  2022\n",
      "西藏自治区  2022\n",
      "吉林省  2022\n",
      "广东省  2022\n",
      "云南省  2023\n",
      "西藏自治区  2023\n",
      "吉林省  2023\n",
      "广东省  2023\n",
      "云南省  2024\n",
      "西藏自治区  2024\n",
      "吉林省  2024\n",
      "广东省  2024\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for year in range(1998,2025):\n",
    "    for p in file_name_list[28:]:\n",
    "        print(p+\" \", year)\n",
    "        text_path = \"../new/policies_cleaned&undivided/reports_clean_0520_withoutG/\"+p+\"/\"+p+str(year)+\"年政府工作报告.txt\"\n",
    "        save_path_forward = \"../new/policies_cleaned&undivided/raw_extraction241101/\"+p+str(year)+\"年政府工作报告.txt\"\n",
    "        if os.path.exists(text_path):\n",
    "            lines_forward, time, raw_len, text_num, lines_summarize = textModification(text_path)\n",
    "            with open(file =save_path_forward, mode='w',encoding='utf - 8') as f_forward:\n",
    "                f_forward.write('提取短语数 '+str(len(lines_forward))+'，全部短语数 '+str(raw_len)+'，字数 '+str(text_num)+'\\n')\n",
    "                #print(lines_forward[0])\n",
    "                #print(lines_forward[0][0],lines_forward[0][1],lines_forward[0][2],lines_forward[0][3])\n",
    "                for index, every_line in enumerate(lines_forward):\n",
    "                    f_forward.write(p+str(year)+' '+str(index)+',')\n",
    "                    for index_key, key in enumerate(every_line[0]):\n",
    "                        f_forward.write(key)\n",
    "                        if index_key < len(every_line[0])-1:\n",
    "                            f_forward.write(' ')\n",
    "                    f_forward.write(',')\n",
    "                    for index_mode1_phrase, mode1_phrase in enumerate(every_line[1]):\n",
    "                        for index_mode1_word, mode1_word in enumerate(mode1_phrase):\n",
    "                            f_forward.write(mode1_word)\n",
    "                            if index_mode1_word < len(mode1_phrase)-1:\n",
    "                                f_forward.write(' ')\n",
    "                        if index_mode1_phrase< len(every_line[1])-1:\n",
    "                            f_forward.write(';')\n",
    "                    f_forward.write(',')\n",
    "                    for index_mode2_phrase, mode2_phrase in enumerate(every_line[2]):\n",
    "                        for index_mode2_word, mode2_word in enumerate(mode2_phrase):\n",
    "                            f_forward.write(mode2_word)\n",
    "                            if index_mode2_word < len(mode2_phrase)-1:\n",
    "                                f_forward.write(' ')\n",
    "                        if index_mode2_phrase< len(every_line[2])-1:\n",
    "                            f_forward.write(';')\n",
    "                    f_forward.write('。')\n",
    "                    f_forward.write(every_line[3])\n",
    "                    f_forward.write('。')\n",
    "                    f_forward.write(every_line[4]) \n",
    "                    f_forward.write('\\n')\n",
    "\n",
    "        else:\n",
    "            print(111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "659a65a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(9+3+2+2+2+2+2+4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbeb6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "NR_list= []\n",
    "\n",
    "wrong_list2 = [\"“\", \"‘\"]\n",
    "\n",
    "list_candidate = []\n",
    "list_candidate2 = []\n",
    "list_candidate3 = []\n",
    "\n",
    "def find_first_matching_element(lst, s):\n",
    "    for item in lst:\n",
    "        if item in s:\n",
    "            return item\n",
    "    return None\n",
    "\n",
    "for year in range(2003,2025):\n",
    "    for p in file_name_list:\n",
    "        #print(p+\" \", year)\n",
    "        text_path = \"../new/policies_cleaned&undivided/reports_clean_0520_withoutG/\"+p+\"/\"+p+str(year)+\"年政府工作报告.txt\"\n",
    "        save_path_forward = \"../new/test/\"+p+str(year)+\"年政府工作报告.txt\"\n",
    "        the_line = False\n",
    "        the_time = True\n",
    "        lines1=[]\n",
    "        lines2_candidate=[]\n",
    "        with open(text_path, encoding='utf-8', errors='ignore') as f:\n",
    "            for line in f:\n",
    "                line=\"\".join(line.split())\n",
    "                ret = re.findall(r'(\\d{4})\\s*[\\./年-]\\s*(\\d{1,2})\\s*[\\./月-]\\s*(\\d{1,2})\\s*日?', line)\n",
    "            #print(line)\n",
    "                if ret and the_time:\n",
    "                    year, month, day = ret[0]\n",
    "                #time = '{}/{}/{}'.format(year, month.lstrip(), day.lstrip())\n",
    "                    time = datetime(int(year), int(month.lstrip()), int(day.lstrip()),hour=0,minute=0,second=0,microsecond=0)\n",
    "                    time = time.strftime(\"%Y-%m-%d\")\n",
    "                    the_time = False\n",
    "                if line.startswith('二&、'):\n",
    "                    the_line = True\n",
    "           #     continue         \n",
    "                if the_line:\n",
    "                    if line.startswith('\\n'):\n",
    "                        a=1\n",
    "                #elif line.count('，') >= 5:\n",
    "                    #lines.append(line)\n",
    "                    #print(line) \n",
    "                #    line = re.split('，|;|。', line)\n",
    "                 #   for l in line:\n",
    "                 #       if l != '\\n' and len(l) !=0:\n",
    "                 #           lines.append(l.replace('\\n', ''))\n",
    "                    else:\n",
    "                        line = line.replace('％', '%')\n",
    "                        line = line.replace('%。左右', '%左右')\n",
    "                        line = line.replace('%。以内', '%以内')\n",
    "                        line_set = re.split('：|；|;|。|，|,|！|包括:', line)\n",
    "                        for l in line_set:\n",
    "                            if l != '\\n' and len(l) !=0:\n",
    "                                l_c = basicClean(l)\n",
    "                                if l_c:\n",
    "                                    if find_first_matching_element(tatal_wrong_list, l_c):\n",
    "                                        wrong_dict[find_first_matching_element(tatal_wrong_list, l_c)].append([l, p+str(year)])\n",
    "                                    #if '把' in l_c: #and '上海' not in l_c:\n",
    "                                    #    list_candidate2.append([l, p+str(year)])\n",
    "                                        \n",
    "                                    #seg_list = TOK([l_c][0])\n",
    "                                    #seg_list = move_stopwords(seg_list, stopwords)\n",
    "                                    #if seg_list:\n",
    "                                        #position = POS([seg_list], conll=False)[0]\n",
    "                                        #for index, p in enumerate(position):\n",
    "                                            #if p == 'NR' and seg_list[index] not in NR_list:\n",
    "                                                #NR_list.append(seg_list[index])\n",
    "                                                \n",
    "\n",
    "                                    #if '让' in l_c and '出让' not in l_c and '转让' not in l_c and '让出' not in l_c:\n",
    "                                    #    list_candidate3.append([l_c, p+str(year)])\n",
    "\n",
    "\n",
    "for i in tatal_wrong_list:\n",
    "    if wrong_dict[i] != []:\n",
    "        with open(\"../new/test/\"+i+\"政府工作报告.txt\", mode='w',encoding='utf - 8') as f:\n",
    "            for every_line in wrong_dict[i]:\n",
    "                #print(every_line)\n",
    "                f.write(every_line[0])\n",
    "                f.write(';  ')\n",
    "                f.write(every_line[1])\n",
    "                f.write('\\n')\n",
    "        \n",
    "#with open(\"../new/test/把政府工作报告.txt\", mode='w',encoding='utf - 8') as f2:\n",
    "#    for every_line2 in list_candidate2:\n",
    "        #print(every_line)\n",
    "#        f2.write(every_line2[0])\n",
    "#        f2.write(';  ')\n",
    "#        f2.write(every_line2[1])\n",
    "#        f2.write('\\n')\n",
    "        \n",
    "#with open(\"../new/test/政府工作报告ex.txt\", mode='w',encoding='utf - 8') as f3:\n",
    "#    for every_line3 in list_candidate3:\n",
    "        #print(every_line)\n",
    "#        f3.write(every_line3[0])\n",
    "#        f3.write(';  ')\n",
    "#        f3.write(every_line3[1])\n",
    "#        f3.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8830808d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['上海市', '云南省', '内蒙古自治区', '北京市', '吉林省', '四川省', '国务院', '天津市', '宁夏回族自治区', '安徽省', '山东省', '山西省', '广东省', '广西', '新疆维吾尔自治区', '江苏省', '江西省', '河北省', '河南省', '浙江省', '海南省', '湖北省', '湖南省', '甘肃省', '福建省', '西藏自治区', '贵州省', '辽宁省', '重庆市', '陕西省', '青海省', '黑龙江省']\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "32\n",
      "[[['新疆维吾尔自治区', '2002-01-15'], ['贵州省', '2002-01-18'], ['青海省', '2002-01-19'], ['内蒙古自治区', '2002-01-22'], ['云南省', '2002-01-23'], ['山西省', '2002-01-23'], ['福建省', '2002-01-23'], ['安徽省', '2002-01-24'], ['河北省', '2002-01-24'], ['河南省', '2002-01-24'], ['重庆市', '2002-01-24'], ['北京市', '2002-01-25'], ['宁夏回族自治区', '2002-01-25'], ['广西', '2002-01-25'], ['江西省', '2002-01-25'], ['浙江省', '2002-01-25'], ['吉林省', '2002-01-26'], ['甘肃省', '2002-01-26'], ['陕西省', '2002-01-26'], ['广东省', '2002-01-28'], ['湖南省', '2002-01-28'], ['黑龙江省', '2002-01-29'], ['海南省', '2002-02-02'], ['江苏省', '2002-02-20'], ['湖北省', '2002-02-21'], ['上海市', '2002-02-22'], ['辽宁省', '2002-02-22'], ['国务院', '2002-03-05'], ['山东省', '2002-03-21'], ['四川省', '2002-04-03'], ['天津市', '2002-04-23'], ['西藏自治区', '2002-05-15']], [['新疆维吾尔自治区', '2003-01-08'], ['重庆市', '2003-01-08'], ['青海省', '2003-01-09'], ['黑龙江省', '2003-01-09'], ['云南省', '2003-01-10'], ['吉林省', '2003-01-10'], ['山西省', '2003-01-10'], ['广西', '2003-01-10'], ['河北省', '2003-01-10'], ['河南省', '2003-01-10'], ['西藏自治区', '2003-01-10'], ['内蒙古自治区', '2003-01-11'], ['宁夏回族自治区', '2003-01-12'], ['贵州省', '2003-01-12'], ['北京市', '2003-01-13'], ['四川省', '2003-01-13'], ['广东省', '2003-01-13'], ['江西省', '2003-01-14'], ['湖南省', '2003-01-14'], ['湖北省', '2003-01-15'], ['浙江省', '2003-01-16'], ['海南省', '2003-01-16'], ['辽宁省', '2003-01-20'], ['福建省', '2003-01-21'], ['陕西省', '2003-01-21'], ['天津市', '2003-01-24'], ['安徽省', '2003-01-26'], ['上海市', '2003-02-16'], ['江苏省', '2003-02-17'], ['甘肃省', '2003-02-24'], ['国务院', '2003-03-05'], ['山东省', '2003-03-27']], [['重庆市', '2004-01-06'], ['广西', '2004-01-07'], ['内蒙古自治区', '2004-01-08'], ['山东省', '2004-01-08'], ['新疆维吾尔自治区', '2004-01-08'], ['河北省', '2004-01-08'], ['福建省', '2004-01-08'], ['贵州省', '2004-01-08'], ['青海省', '2004-01-08'], ['甘肃省', '2004-01-09'], ['湖南省', '2004-01-10'], ['天津市', '2004-01-12'], ['安徽省', '2004-01-12'], ['上海市', '2004-01-22'], ['陕西省', '2004-02-02'], ['湖北省', '2004-02-03'], ['宁夏回族自治区', '2004-02-06'], ['江西省', '2004-02-09'], ['黑龙江省', '2004-02-09'], ['吉林省', '2004-02-10'], ['广东省', '2004-02-10'], ['河南省', '2004-02-10'], ['浙江省', '2004-02-10'], ['云南省', '2004-02-12'], ['四川省', '2004-02-12'], ['山西省', '2004-02-12'], ['北京市', '2004-02-16'], ['辽宁省', '2004-02-22'], ['江苏省', '2004-02-23'], ['海南省', '2004-02-24'], ['国务院', '2004-03-05'], ['西藏自治区', '2004-05-17']], [['内蒙古自治区', '2005-01-08'], ['甘肃省', '2005-01-10'], ['河北省', '2005-01-11'], ['新疆维吾尔自治区', '2005-01-12'], ['重庆市', '2005-01-12'], ['青海省', '2005-01-16'], ['福建省', '2005-01-18'], ['云南省', '2005-01-19'], ['江苏省', '2005-01-19'], ['广西', '2005-01-20'], ['贵州省', '2005-01-20'], ['四川省', '2005-01-21'], ['上海市', '2005-01-22'], ['北京市', '2005-01-23'], ['天津市', '2005-01-23'], ['广东省', '2005-01-23'], ['山西省', '2005-01-24'], ['江西省', '2005-01-24'], ['湖北省', '2005-01-24'], ['吉林省', '2005-01-25'], ['宁夏回族自治区', '2005-01-25'], ['安徽省', '2005-01-25'], ['河南省', '2005-01-25'], ['黑龙江省', '2005-01-25'], ['湖南省', '2005-01-27'], ['海南省', '2005-01-28'], ['陕西省', '2005-01-31'], ['山东省', '2005-02-22'], ['辽宁省', '2005-02-23'], ['浙江省', '2005-02-27'], ['国务院', '2005-03-05'], ['西藏自治区', '2005-05-08']], [['湖北省', '2006-01-06'], ['福建省', '2006-01-08'], ['山西省', '2006-01-10'], ['重庆市', '2006-01-11'], ['广西', '2006-01-12'], ['西藏自治区', '2006-01-12'], ['青海省', '2006-01-14'], ['上海市', '2006-01-15'], ['内蒙古自治区', '2006-01-15'], ['北京市', '2006-01-15'], ['四川省', '2006-01-15'], ['山东省', '2006-01-15'], ['河南省', '2006-01-15'], ['甘肃省', '2006-01-15'], ['云南省', '2006-01-16'], ['天津市', '2006-01-16'], ['江苏省', '2006-01-16'], ['浙江省', '2006-01-16'], ['海南省', '2006-01-16'], ['贵州省', '2006-01-16'], ['新疆维吾尔自治区', '2006-01-17'], ['湖南省', '2006-01-17'], ['陕西省', '2006-01-17'], ['辽宁省', '2006-01-20'], ['黑龙江省', '2006-02-07'], ['吉林省', '2006-02-08'], ['宁夏回族自治区', '2006-02-08'], ['河北省', '2006-02-16'], ['广东省', '2006-02-22'], ['安徽省', '2006-02-23'], ['国务院', '2006-03-05'], ['江西省', '2006-03-28']], [['西藏自治区', '2007-01-12'], ['河北省', '2007-01-16'], ['吉林省', '2007-01-20'], ['新疆维吾尔自治区', '2007-01-22'], ['贵州省', '2007-01-22'], ['安徽省', '2007-01-23'], ['辽宁省', '2007-01-23'], ['重庆市', '2007-01-23'], ['福建省', '2007-01-24'], ['内蒙古自治区', '2007-01-25'], ['江西省', '2007-01-25'], ['黑龙江省', '2007-01-25'], ['云南省', '2007-01-26'], ['北京市', '2007-01-26'], ['四川省', '2007-01-26'], ['广西', '2007-01-26'], ['河南省', '2007-01-26'], ['甘肃省', '2007-01-26'], ['江苏省', '2007-01-27'], ['上海市', '2007-01-28'], ['天津市', '2007-01-28'], ['山西省', '2007-01-29'], ['浙江省', '2007-01-29'], ['湖南省', '2007-01-29'], ['陕西省', '2007-01-29'], ['湖北省', '2007-02-01'], ['广东省', '2007-02-02'], ['宁夏回族自治区', '2007-02-03'], ['海南省', '2007-02-05'], ['山东省', '2007-02-09'], ['青海省', '2007-02-09'], ['国务院', '2007-03-05']], [['吉林省', '2008-01-08'], ['青海省', '2008-01-10'], ['新疆维吾尔自治区', '2008-01-15'], ['宁夏回族自治区', '2008-01-16'], ['山西省', '2008-01-16'], ['河南省', '2008-01-16'], ['浙江省', '2008-01-16'], ['西藏自治区', '2008-01-16'], ['陕西省', '2008-01-16'], ['广东省', '2008-01-17'], ['云南省', '2008-01-18'], ['福建省', '2008-01-18'], ['贵州省', '2008-01-18'], ['广西', '2008-01-19'], ['北京市', '2008-01-20'], ['山东省', '2008-01-20'], ['湖南省', '2008-01-20'], ['甘肃省', '2008-01-20'], ['重庆市', '2008-01-20'], ['辽宁省', '2008-01-21'], ['天津市', '2008-01-22'], ['江西省', '2008-01-23'], ['河北省', '2008-01-23'], ['黑龙江省', '2008-01-23'], ['上海市', '2008-01-24'], ['四川省', '2008-01-24'], ['海南省', '2008-01-24'], ['安徽省', '2008-01-25'], ['江苏省', '2008-01-25'], ['湖北省', '2008-01-25'], ['内蒙古自治区', '2008-01-31'], ['国务院', '2008-03-05']], [['新疆维吾尔自治区', '2009-01-07'], ['内蒙古自治区', '2009-01-08'], ['河北省', '2009-01-08'], ['重庆市', '2009-01-08'], ['广西', '2009-01-10'], ['福建省', '2009-01-10'], ['天津市', '2009-01-11'], ['山西省', '2009-01-11'], ['北京市', '2009-01-12'], ['河南省', '2009-01-12'], ['海南省', '2009-01-12'], ['湖南省', '2009-01-12'], ['甘肃省', '2009-01-12'], ['贵州省', '2009-01-12'], ['青海省', '2009-01-12'], ['上海市', '2009-01-13'], ['湖北省', '2009-01-13'], ['陕西省', '2009-01-13'], ['宁夏回族自治区', '2009-01-14'], ['安徽省', '2009-01-14'], ['西藏自治区', '2009-01-14'], ['辽宁省', '2009-01-14'], ['黑龙江省', '2009-01-14'], ['四川省', '2009-01-15'], ['浙江省', '2009-01-16'], ['吉林省', '2009-01-17'], ['江苏省', '2009-02-05'], ['云南省', '2009-02-07'], ['江西省', '2009-02-12'], ['山东省', '2009-02-13'], ['广东省', '2009-02-13'], ['国务院', '2009-03-05']], [['西藏自治区', '2010-01-10'], ['新疆维吾尔自治区', '2010-01-12'], ['天津市', '2010-01-16'], ['河北省', '2010-01-18'], ['吉林省', '2010-01-19'], ['内蒙古自治区', '2010-01-20'], ['贵州省', '2010-01-20'], ['重庆市', '2010-01-20'], ['云南省', '2010-01-22'], ['辽宁省', '2010-01-22'], ['北京市', '2010-01-25'], ['安徽省', '2010-01-25'], ['山东省', '2010-01-25'], ['河南省', '2010-01-25'], ['海南省', '2010-01-25'], ['湖南省', '2010-01-25'], ['甘肃省', '2010-01-25'], ['福建省', '2010-01-25'], ['陕西省', '2010-01-25'], ['青海省', '2010-01-25'], ['黑龙江省', '2010-01-25'], ['上海市', '2010-01-26'], ['四川省', '2010-01-26'], ['山西省', '2010-01-26'], ['江苏省', '2010-01-26'], ['江西省', '2010-01-26'], ['浙江省', '2010-01-26'], ['湖北省', '2010-01-26'], ['广西', '2010-01-27'], ['广东省', '2010-01-29'], ['宁夏回族自治区', '2010-02-02'], ['国务院', '2010-03-05']], [['重庆市', '2011-01-09'], ['西藏自治区', '2011-01-10'], ['河北省', '2011-01-12'], ['甘肃省', '2011-01-13'], ['福建省', '2011-01-13'], ['新疆维吾尔自治区', '2011-01-14'], ['上海市', '2011-01-16'], ['内蒙古自治区', '2011-01-16'], ['北京市', '2011-01-16'], ['天津市', '2011-01-16'], ['浙江省', '2011-01-16'], ['广西', '2011-01-17'], ['河南省', '2011-01-17'], ['陕西省', '2011-01-17'], ['青海省', '2011-01-17'], ['四川省', '2011-01-18'], ['宁夏回族自治区', '2011-01-18'], ['安徽省', '2011-01-18'], ['贵州省', '2011-01-18'], ['山西省', '2011-01-19'], ['湖南省', '2011-01-20'], ['黑龙江省', '2011-01-20'], ['云南省', '2011-01-21'], ['辽宁省', '2011-01-21'], ['广东省', '2011-01-22'], ['江苏省', '2011-02-10'], ['吉林省', '2011-02-12'], ['山东省', '2011-02-12'], ['江西省', '2011-02-14'], ['湖北省', '2011-02-20'], ['海南省', '2011-02-21'], ['国务院', '2011-03-05']], [['河北省', '2012-01-06'], ['广西', '2012-01-08'], ['河南省', '2012-01-08'], ['重庆市', '2012-01-08'], ['天津市', '2012-01-09'], ['甘肃省', '2012-01-09'], ['西藏自治区', '2012-01-09'], ['黑龙江省', '2012-01-09'], ['四川省', '2012-01-10'], ['新疆维吾尔自治区', '2012-01-10'], ['贵州省', '2012-01-10'], ['上海市', '2012-01-11'], ['宁夏回族自治区', '2012-01-11'], ['山西省', '2012-01-11'], ['湖北省', '2012-01-11'], ['湖南省', '2012-01-11'], ['北京市', '2012-01-12'], ['浙江省', '2012-01-12'], ['福建省', '2012-01-12'], ['辽宁省', '2012-01-12'], ['陕西省', '2012-01-12'], ['广东省', '2012-01-13'], ['青海省', '2012-01-13'], ['吉林省', '2012-02-01'], ['江西省', '2012-02-01'], ['江苏省', '2012-02-09'], ['海南省', '2012-02-09'], ['云南省', '2012-02-11'], ['安徽省', '2012-02-11'], ['内蒙古自治区', '2012-02-15'], ['山东省', '2012-02-19'], ['国务院', '2012-03-05']], [['江苏省', '2013-01-20'], ['云南省', '2013-01-21'], ['河南省', '2013-01-21'], ['北京市', '2013-01-22'], ['安徽省', '2013-01-22'], ['广西', '2013-01-22'], ['湖北省', '2013-01-22'], ['宁夏回族自治区', '2013-01-23'], ['山西省', '2013-01-23'], ['江西省', '2013-01-23'], ['甘肃省', '2013-01-23'], ['青海省', '2013-01-23'], ['西藏自治区', '2013-01-24'], ['辽宁省', '2013-01-24'], ['内蒙古自治区', '2013-01-25'], ['四川省', '2013-01-25'], ['山东省', '2013-01-25'], ['广东省', '2013-01-25'], ['浙江省', '2013-01-25'], ['吉林省', '2013-01-26'], ['天津市', '2013-01-26'], ['新疆维吾尔自治区', '2013-01-26'], ['河北省', '2013-01-26'], ['湖南省', '2013-01-26'], ['贵州省', '2013-01-26'], ['重庆市', '2013-01-26'], ['黑龙江省', '2013-01-26'], ['上海市', '2013-01-27'], ['海南省', '2013-01-27'], ['福建省', '2013-01-27'], ['陕西省', '2013-01-27'], ['国务院', '2013-03-05']], [['宁夏回族自治区', '2014-01-07'], ['河北省', '2014-01-08'], ['西藏自治区', '2014-01-10'], ['甘肃省', '2014-01-13'], ['陕西省', '2014-01-14'], ['内蒙古自治区', '2014-01-15'], ['北京市', '2014-01-16'], ['广东省', '2014-01-16'], ['广西', '2014-01-16'], ['新疆维吾尔自治区', '2014-01-16'], ['河南省', '2014-01-16'], ['浙江省', '2014-01-16'], ['贵州省', '2014-01-16'], ['山东省', '2014-01-17'], ['湖北省', '2014-01-17'], ['辽宁省', '2014-01-17'], ['四川省', '2014-01-18'], ['天津市', '2014-01-18'], ['山西省', '2014-01-18'], ['江苏省', '2014-01-19'], ['重庆市', '2014-01-19'], ['青海省', '2014-01-19'], ['黑龙江省', '2014-01-19'], ['云南省', '2014-01-20'], ['吉林省', '2014-01-21'], ['江西省', '2014-01-21'], ['福建省', '2014-01-22'], ['上海市', '2014-01-25'], ['安徽省', '2014-02-09'], ['海南省', '2014-02-09'], ['湖南省', '2014-02-10'], ['国务院', '2014-03-05']], [['河北省', '2015-01-08'], ['西藏自治区', '2015-01-18'], ['重庆市', '2015-01-18'], ['宁夏回族自治区', '2015-01-20'], ['新疆维吾尔自治区', '2015-01-20'], ['浙江省', '2015-01-21'], ['青海省', '2015-01-22'], ['北京市', '2015-01-23'], ['天津市', '2015-01-25'], ['陕西省', '2015-01-25'], ['云南省', '2015-01-26'], ['内蒙古自治区', '2015-01-26'], ['安徽省', '2015-01-26'], ['贵州省', '2015-01-26'], ['山东省', '2015-01-27'], ['广西', '2015-01-27'], ['江苏省', '2015-01-27'], ['江西省', '2015-01-27'], ['湖北省', '2015-01-27'], ['湖南省', '2015-01-27'], ['辽宁省', '2015-01-27'], ['黑龙江省', '2015-01-27'], ['四川省', '2015-01-28'], ['山西省', '2015-01-28'], ['河南省', '2015-01-28'], ['甘肃省', '2015-01-28'], ['福建省', '2015-01-28'], ['吉林省', '2015-02-09'], ['广东省', '2015-02-09'], ['海南省', '2015-02-09'], ['上海市', '2015-02-10'], ['国务院', '2015-03-05']], [['河北省', '2016-01-08'], ['宁夏回族自治区', '2016-01-11'], ['新疆维吾尔自治区', '2016-01-11'], ['福建省', '2016-01-11'], ['甘肃省', '2016-01-16'], ['北京市', '2016-01-22'], ['内蒙古自治区', '2016-01-23'], ['云南省', '2016-01-24'], ['天津市', '2016-01-24'], ['山东省', '2016-01-24'], ['广西', '2016-01-24'], ['江苏省', '2016-01-24'], ['浙江省', '2016-01-24'], ['重庆市', '2016-01-24'], ['陕西省', '2016-01-24'], ['四川省', '2016-01-25'], ['广东省', '2016-01-25'], ['江西省', '2016-01-25'], ['河南省', '2016-01-25'], ['湖南省', '2016-01-25'], ['青海省', '2016-01-25'], ['吉林省', '2016-01-26'], ['海南省', '2016-01-26'], ['湖北省', '2016-01-26'], ['贵州省', '2016-01-26'], ['辽宁省', '2016-01-26'], ['山西省', '2016-01-27'], ['西藏自治区', '2016-01-27'], ['黑龙江省', '2016-01-27'], ['上海市', '2016-02-02'], ['安徽省', '2016-02-17'], ['国务院', '2016-03-05']], [['河北省', '2017-01-08'], ['新疆维吾尔自治区', '2017-01-09'], ['宁夏回族自治区', '2017-01-10'], ['西藏自治区', '2017-01-10'], ['广西', '2017-01-13'], ['甘肃省', '2017-01-13'], ['内蒙古自治区', '2017-01-14'], ['北京市', '2017-01-14'], ['山西省', '2017-01-14'], ['湖南省', '2017-01-14'], ['上海市', '2017-01-15'], ['吉林省', '2017-01-15'], ['天津市', '2017-01-15'], ['湖北省', '2017-01-15'], ['重庆市', '2017-01-15'], ['陕西省', '2017-01-15'], ['青海省', '2017-01-15'], ['云南省', '2017-01-16'], ['四川省', '2017-01-16'], ['安徽省', '2017-01-16'], ['江西省', '2017-01-16'], ['河南省', '2017-01-16'], ['浙江省', '2017-01-16'], ['贵州省', '2017-01-16'], ['黑龙江省', '2017-01-16'], ['辽宁省', '2017-01-17'], ['福建省', '2017-01-18'], ['广东省', '2017-01-19'], ['山东省', '2017-02-06'], ['江苏省', '2017-02-06'], ['海南省', '2017-02-20'], ['国务院', '2017-03-05']], [['安徽省', '2018-01-22'], ['新疆维吾尔自治区', '2018-01-22'], ['上海市', '2018-01-23'], ['江西省', '2018-01-23'], ['内蒙古自治区', '2018-01-24'], ['北京市', '2018-01-24'], ['天津市', '2018-01-24'], ['河南省', '2018-01-24'], ['湖北省', '2018-01-24'], ['湖南省', '2018-01-24'], ['甘肃省', '2018-01-24'], ['西藏自治区', '2018-01-24'], ['云南省', '2018-01-25'], ['山东省', '2018-01-25'], ['山西省', '2018-01-25'], ['广东省', '2018-01-25'], ['广西', '2018-01-25'], ['河北省', '2018-01-25'], ['浙江省', '2018-01-25'], ['陕西省', '2018-01-25'], ['青海省', '2018-01-25'], ['黑龙江省', '2018-01-25'], ['吉林省', '2018-01-26'], ['四川省', '2018-01-26'], ['宁夏回族自治区', '2018-01-26'], ['江苏省', '2018-01-26'], ['海南省', '2018-01-26'], ['福建省', '2018-01-26'], ['贵州省', '2018-01-26'], ['重庆市', '2018-01-26'], ['辽宁省', '2018-01-27'], ['国务院', '2018-03-05']], [['西藏自治区', '2019-01-10'], ['北京市', '2019-01-14'], ['四川省', '2019-01-14'], ['天津市', '2019-01-14'], ['安徽省', '2019-01-14'], ['新疆维吾尔自治区', '2019-01-14'], ['江苏省', '2019-01-14'], ['河北省', '2019-01-14'], ['湖北省', '2019-01-14'], ['福建省', '2019-01-14'], ['黑龙江省', '2019-01-14'], ['河南省', '2019-01-16'], ['辽宁省', '2019-01-16'], ['内蒙古自治区', '2019-01-26'], ['吉林省', '2019-01-26'], ['山西省', '2019-01-26'], ['广西', '2019-01-26'], ['湖南省', '2019-01-26'], ['甘肃省', '2019-01-26'], ['云南省', '2019-01-27'], ['宁夏回族自治区', '2019-01-27'], ['江西省', '2019-01-27'], ['浙江省', '2019-01-27'], ['海南省', '2019-01-27'], ['贵州省', '2019-01-27'], ['重庆市', '2019-01-27'], ['陕西省', '2019-01-27'], ['青海省', '2019-01-27'], ['广东省', '2019-01-28'], ['上海市', '2019-02-03'], ['山东省', '2019-02-14'], ['国务院', '2019-03-05']], [['新疆维吾尔自治区', '2020-01-06'], ['河北省', '2020-01-07'], ['西藏自治区', '2020-01-07'], ['河南省', '2020-01-10'], ['甘肃省', '2020-01-10'], ['宁夏回族自治区', '2020-01-11'], ['福建省', '2020-01-11'], ['重庆市', '2020-01-11'], ['内蒙古自治区', '2020-01-12'], ['北京市', '2020-01-12'], ['吉林省', '2020-01-12'], ['安徽省', '2020-01-12'], ['广西', '2020-01-12'], ['浙江省', '2020-01-12'], ['湖北省', '2020-01-12'], ['黑龙江省', '2020-01-12'], ['山西省', '2020-01-13'], ['湖南省', '2020-01-13'], ['天津市', '2020-01-14'], ['广东省', '2020-01-14'], ['辽宁省', '2020-01-14'], ['上海市', '2020-01-15'], ['江苏省', '2020-01-15'], ['江西省', '2020-01-15'], ['贵州省', '2020-01-15'], ['陕西省', '2020-01-15'], ['青海省', '2020-01-15'], ['海南省', '2020-01-16'], ['山东省', '2020-01-18'], ['四川省', '2020-05-09'], ['云南省', '2020-05-10'], ['国务院', '2020-05-22']], [['河南省', '2021-01-18'], ['山西省', '2021-01-20'], ['西藏自治区', '2021-01-20'], ['广西', '2021-01-21'], ['重庆市', '2021-01-21'], ['北京市', '2021-01-23'], ['上海市', '2021-01-24'], ['广东省', '2021-01-24'], ['海南省', '2021-01-24'], ['湖北省', '2021-01-24'], ['福建省', '2021-01-24'], ['吉林省', '2021-01-25'], ['天津市', '2021-01-25'], ['湖南省', '2021-01-25'], ['甘肃省', '2021-01-25'], ['贵州省', '2021-01-25'], ['云南省', '2021-01-26'], ['内蒙古自治区', '2021-01-26'], ['江苏省', '2021-01-26'], ['江西省', '2021-01-26'], ['浙江省', '2021-01-26'], ['陕西省', '2021-01-26'], ['安徽省', '2021-01-28'], ['辽宁省', '2021-01-28'], ['宁夏回族自治区', '2021-01-29'], ['四川省', '2021-01-30'], ['青海省', '2021-01-30'], ['新疆维吾尔自治区', '2021-02-01'], ['山东省', '2021-02-02'], ['河北省', '2021-02-19'], ['黑龙江省', '2021-02-19'], ['国务院', '2021-03-05']], [['西藏自治区', '2022-01-04'], ['北京市', '2022-01-06'], ['河南省', '2022-01-06'], ['安徽省', '2022-01-17'], ['广西', '2022-01-17'], ['江西省', '2022-01-17'], ['河北省', '2022-01-17'], ['浙江省', '2022-01-17'], ['湖南省', '2022-01-17'], ['甘肃省', '2022-01-17'], ['重庆市', '2022-01-17'], ['四川省', '2022-01-18'], ['陕西省', '2022-01-19'], ['上海市', '2022-01-20'], ['云南省', '2022-01-20'], ['宁夏回族自治区', '2022-01-20'], ['山西省', '2022-01-20'], ['广东省', '2022-01-20'], ['江苏省', '2022-01-20'], ['湖北省', '2022-01-20'], ['贵州省', '2022-01-20'], ['辽宁省', '2022-01-20'], ['内蒙古自治区', '2022-01-21'], ['海南省', '2022-01-21'], ['青海省', '2022-01-21'], ['福建省', '2022-01-22'], ['山东省', '2022-01-23'], ['新疆维吾尔自治区', '2022-01-23'], ['黑龙江省', '2022-01-23'], ['吉林省', '2022-01-24'], ['天津市', '2022-02-12'], ['国务院', '2022-03-05']]]\n"
     ]
    }
   ],
   "source": [
    "file_name_list = os.listdir('province_annual_data')\n",
    "print(file_name_list)\n",
    "time_list = []\n",
    "for year in range(2002,2023):\n",
    "    year_list = []\n",
    "    for p in file_name_list:\n",
    "        #print(p+\" \", year)\n",
    "        text_path = \"province_annual_data/\"+p+\"/\"+p+str(year)+\"年政府工作报告.txt\"\n",
    "        with open(text_path, encoding='utf-8', errors='ignore') as f:\n",
    "            the_time = True\n",
    "            for line in f:\n",
    "                line=\"\".join(line.split())\n",
    "                ret = re.findall(r'(\\d{4})\\s*[\\./年-]\\s*(\\d{1,2})\\s*[\\./月-]\\s*(\\d{1,2})\\s*日?', line)\n",
    "                if ret and the_time:\n",
    "                    year, month, day = ret[0]\n",
    "                    time = datetime(int(year), int(month.lstrip()), int(day.lstrip()),hour=0,minute=0,second=0,microsecond=0)\n",
    "                    time = time.strftime(\"%Y-%m-%d\")\n",
    "                    year_list.append([p, time])\n",
    "                    break\n",
    "    time_list.append(sorted(year_list, key=lambda x: x[1], reverse=False))\n",
    "    print(len(year_list))\n",
    "print(time_list)\n",
    "with open(file ='time_table.txt', mode='w',encoding='utf - 8') as t_w:\n",
    "    for years in time_list:\n",
    "        for couple in years:\n",
    "            t_w.write(couple[0])\n",
    "            t_w.write(' ')\n",
    "            t_w.write(couple[1])\n",
    "            t_w.write(',')\n",
    "        t_w.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd7f6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 16:15:02.468 | DEBUG    | text2vec.sentence_model:__init__:74 - Use device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: 如何更换花呗绑定银行卡\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "花呗更改绑定银行卡 (Score: 0.8551)\n",
      "我什么时候开通了花呗 (Score: 0.7212)\n",
      "A man is eating food. (Score: 0.3118)\n",
      "A man is eating a piece of bread. (Score: 0.2992)\n",
      "A monkey is playing drums. (Score: 0.2922)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: A man is eating pasta.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A man is eating food. (Score: 0.7840)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.6906)\n",
      "A man is eating a piece of bread. (Score: 0.6831)\n",
      "A man is riding a horse. (Score: 0.6515)\n",
      "Two men pushed carts through the woods. (Score: 0.5270)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Someone in a gorilla costume is playing a set of drums.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A monkey is playing drums. (Score: 0.6758)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.6351)\n",
      "The girl is carrying a baby. (Score: 0.5438)\n",
      "A man is riding a horse. (Score: 0.5002)\n",
      "A man is eating a piece of bread. (Score: 0.4916)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: A cheetah chases prey on across a field.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "A cheetah is running behind its prey. (Score: 0.6736)\n",
      "A man is riding a white horse on an enclosed ground. (Score: 0.5731)\n",
      "A monkey is playing drums. (Score: 0.4977)\n",
      "The girl is carrying a baby. (Score: 0.4570)\n",
      "A man is riding a horse. (Score: 0.4287)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ffe54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
