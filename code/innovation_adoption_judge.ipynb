{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41f3b1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "355990it [00:14, 25332.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import jieba\n",
    "import hanlp\n",
    "import time\n",
    "import pandas as pd\n",
    "#import torch\n",
    "import openpyxl\n",
    "#from torch import cosine_similarity\n",
    "\n",
    "#from text2vec import SentenceModel, cos_sim, semantic_search\n",
    "from datetime import datetime\n",
    "#从自写工具包中导入：修饰词判定、修饰词查找\n",
    "from utils import modifier_judge, modifier_search\n",
    "\n",
    "#device = torch.device('cuda')\n",
    "#use_gpu = torch.cuda.is_available()\n",
    "\n",
    "def read_vectors(path, topn):  # read top n word vectors, i.e. top is 10000\n",
    "    lines_num, dim = 0, 0\n",
    "    vectors = {}\n",
    "    iw = []\n",
    "    wi = {}\n",
    "    #num_file = sum([1 for i in open(path, encoding='utf-8', errors='ignore')])  \n",
    "    #print(num_file)\n",
    "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        first_line = True\n",
    "        for line in tqdm(f):\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                dim = int(line.rstrip().split()[1])\n",
    "                continue\n",
    "            lines_num += 1\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            vectors[tokens[0]] = np.expand_dims(np.asarray([float(x) for x in tokens[1:]]),axis=0)\n",
    "            #vectors[tokens[0]] = torch.tensor([float(x) for x in tokens[1:]], device=device).unsqueeze(0)\n",
    "            iw.append(tokens[0])\n",
    "            if topn != 0 and lines_num >= topn:\n",
    "                break\n",
    "    for i, w in enumerate(iw):\n",
    "        wi[w] = i\n",
    "    return vectors, iw, wi, dim\n",
    "\n",
    "vectors_path = \"../test/sgns.renmin.bigram\"\n",
    "topn = 0\n",
    "vectors, iw, wi, dim = read_vectors(vectors_path, topn) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e5154f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n",
      "000\n",
      "-0.5\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(vec1, vec2):\n",
    "    if vec1.all() == 0 or vec2.all() == 0:\n",
    "        return np.zeros([1])[0]\n",
    "    else:\n",
    "        return (vec1.dot(vec2[0]) / (np.linalg.norm(vec1[0]) * np.linalg.norm(vec2[0])))[0]\n",
    "\n",
    "\n",
    "\n",
    "def word2vec(word1):\n",
    "    if word1 in vectors:\n",
    "        return vectors[word1]\n",
    "    else:\n",
    "        #print('not find')\n",
    "        first_line = True\n",
    "        for character in word1:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                if character in vectors:\n",
    "                    vector = vectors[character]\n",
    "                else: \n",
    "                    vector = np.zeros([1, 300])\n",
    "                continue\n",
    "            if character in vectors:\n",
    "                vector = np.append(vector,vectors[character], axis = 0)\n",
    "        return np.mean(vector, axis = 0).reshape(1,300)\n",
    "\n",
    "def words2vec(word_list):\n",
    "    first_line = True\n",
    "    for word in word_list:\n",
    "        if first_line:\n",
    "            first_line = False\n",
    "            vector = word2vec(word)\n",
    "            continue\n",
    "        vector =  np.append(vector,word2vec(word), axis = 0)\n",
    "    return np.mean(vector, axis = 0).reshape(1,300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d07231c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_phrases(path):\n",
    "    with open (path, 'r',  encoding='utf-8') as phrases_read:\n",
    "        phrases = []\n",
    "        first_line = True\n",
    "        for line in phrases_read:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                line_number = line\n",
    "                continue\n",
    "            cut = line.replace('\\n', '').split('。')\n",
    "            phrase = cut[0].split(',')\n",
    "            sentence = cut[1]+' '+cut[2]\n",
    "            phrase = [tuple(x.split(';')) for x in phrase]\n",
    "            phrase.append(sentence)\n",
    "            phrases.append(tuple(phrase))\n",
    "    return phrases, line_number\n",
    "\n",
    "\n",
    "def simlirity_for_phase_mode1(phase_before, phase_now):\n",
    "    #print(phase_before, phase_now)\n",
    "    sim_v = cos_sim(word2vec(phase_before[0]),word2vec(phase_now[0]))\n",
    "    sim_key = cos_sim(word2vec(phase_before[1]),word2vec(phase_now[1]))\n",
    "    sim_modify = np.zeros([1])[0]\n",
    "    if len(phase_now)>2:\n",
    "        if len(phase_before)>2:\n",
    "            for word_now in phase_now[2:]:\n",
    "                sim_modify_tem = np.zeros([1])[0]\n",
    "                for word_before in phase_before[2:]:\n",
    "                    tem_sim = cos_sim(word2vec(word_before),word2vec(word_now))\n",
    "                    if tem_sim > sim_modify_tem:\n",
    "                        sim_modify_tem = tem_sim\n",
    "                sim_modify = sim_modify + sim_modify_tem\n",
    "            sim_modify = sim_modify/len(phase_now[2:])\n",
    "            return 0.1*sim_v+0.5*sim_key+0.4*sim_modify\n",
    "        else:\n",
    "            return 0.1*sim_v+0.6*sim_key\n",
    "    else:\n",
    "        return 0.14*sim_v+0.86*sim_key\n",
    "    \n",
    "def simlirity_for_phase_mode2(phase_before, phase_now):\n",
    "    sim_key = cos_sim(word2vec(phase_before[0]),word2vec(phase_now[0]))\n",
    "    sim_modify = np.zeros([1])[0]\n",
    "    if len(phase_now)>1:\n",
    "        if len(phase_before)>1:\n",
    "            sim_modify = cos_sim(words2vec(phase_before[1:]),words2vec(phase_now[1:]))\n",
    "            return 0.4*sim_key+0.6*sim_modify\n",
    "        else:\n",
    "            return 0.4*sim_key\n",
    "    else:\n",
    "        return sim_key\n",
    "\n",
    "def simlirity_for_phrases(phrases_before, phrases_now):\n",
    "    sum1 = np.zeros([1])[0]\n",
    "    sum2 = np.zeros([1])[0]\n",
    "    count1 = len(phrases_now[0])\n",
    "    count2 = len(phrases_now[1])\n",
    "    another_mode = False\n",
    "    if len(phrases_now[1])> len(phrases_before[1]):\n",
    "        another_mode = True\n",
    "    temp_phrases_now = [list(i_now) for i_now in phrases_now]\n",
    "    temp_phrases_before = [list(i_before) for i_before in phrases_before]\n",
    "    #print(phrases_before, phrases_now)\n",
    "    #print(temp_phrases_before, temp_phrases_now)\n",
    "    if phrases_now[0][0] and phrases_now[1][0]:\n",
    "        the_type = True\n",
    "    else:\n",
    "        the_type = False\n",
    "    #现在的无mode2，过去无mode2\n",
    "    #print(phrases_before, phrases_now)\n",
    "    #print(phrases_before[1][0], phrases_now[1])\n",
    "    while temp_phrases_now[0][0] and temp_phrases_before[0][0]:\n",
    "        sim1_2 = np.zeros([1])[0]\n",
    "        for index1_1, phrase_now in enumerate(temp_phrases_now[0]):            \n",
    "            for index1_2, phrase_before in enumerate(temp_phrases_before[0]):\n",
    "                sim_temp1 = simlirity_for_phase_mode1(phrase_before.split(' '), phrase_now.split(' '))\n",
    "                #print(sim_temp1)\n",
    "                if sim_temp1 > sim1_2:\n",
    "                    pos1_1 = phrase_now\n",
    "                    pos1_2 = phrase_before\n",
    "                    sim1_2 = sim_temp1\n",
    "        sum1 = sum1 + sim1_2\n",
    "        if len(temp_phrases_now[0]) == 1 or len(temp_phrases_before[0]) == 1:\n",
    "            break\n",
    "        temp_phrases_now[0].remove(pos1_1)\n",
    "        temp_phrases_before[0].remove(pos1_2)\n",
    "    \n",
    "    while temp_phrases_now[1][0] and temp_phrases_before[1][0]:\n",
    "        sim2_2 = np.zeros([1])[0]-0.5\n",
    "        pos2_1 = ''\n",
    "        pos2_2 = ''\n",
    "        for index2_1, phrase_now2 in enumerate(temp_phrases_now[1]):\n",
    "            for index2_2, phrase_before2 in enumerate(temp_phrases_before[1]):\n",
    "                sim_temp2 = simlirity_for_phase_mode2(phrase_before2.split(' '), phrase_now2.split(' '))\n",
    "                if sim_temp2 > sim2_2:\n",
    "                    pos2_1 = phrase_now2\n",
    "                    pos2_2 = phrase_before2\n",
    "                    sim2_2 = sim_temp2\n",
    "        sum2 = sum2 + sim2_2\n",
    "        if len(temp_phrases_now[1]) == 1 or len(temp_phrases_before[1]) == 1:\n",
    "            if another_mode and pos2_1 in temp_phrases_now[1]:\n",
    "                temp_phrases_now[1].remove(pos2_1)\n",
    "            break\n",
    "        try:\n",
    "            temp_phrases_now[1].remove(pos2_1)\n",
    "            temp_phrases_before[1].remove(pos2_2)\n",
    "        except:\n",
    "            print(phrases_before, phrases_now)\n",
    "            print(temp_phrases_before, temp_phrases_now, pos2_1, pos2_2)\n",
    "            break\n",
    "            \n",
    "    if another_mode:\n",
    "        while temp_phrases_now[1][0] and temp_phrases_before[0][0]:\n",
    "            sim2_3 = np.zeros([1])[0]\n",
    "            for index23_1, phrase_now23 in enumerate(temp_phrases_now[1]):\n",
    "                for index23_2, phrase_before23 in enumerate(temp_phrases_before[0]):\n",
    "                    sim_temp23 = simlirity_for_phase_mode2(phrase_before23.split(' ')[1:], phrase_now23.split(' '))\n",
    "                    if sim_temp23 > sim2_3:\n",
    "                        pos23_1 = phrase_now23\n",
    "                        pos23_2 = phrase_before23\n",
    "                        sim2_3 = sim_temp23\n",
    "            sum2 = sum2 + sim2_3\n",
    "            if len(temp_phrases_now[1]) == 1 or len(temp_phrases_before[0]) == 1:\n",
    "                break\n",
    "            try:\n",
    "                temp_phrases_now[1].remove(pos23_1)\n",
    "            except:\n",
    "                print(phrases_before, phrases_now)\n",
    "                print(temp_phrases_before, temp_phrases_now, pos23_1, pos23_2)\n",
    "                break\n",
    "    \n",
    "    if count1 > 0 :\n",
    "        sim1 = sum1/count1\n",
    "    if count2 > 0 :\n",
    "        sim2 = sum2/count2\n",
    "    \n",
    "    if the_type:\n",
    "        return 0.77*sim1 + 0.23*sim2\n",
    "    else:\n",
    "        return sim1 + sim2\n",
    "        \n",
    "        \n",
    "def similirity_for_corpus(corpus_before, corpus_now):\n",
    "    inno_dict={ \"α=0.88\":[], \"α=0.9\":[]}\n",
    "    sim_dict={ \"α=0.88\":[], \"α=0.9\":[]}\n",
    "    defussion_dict = {\"α=0.88\":[], \"α=0.9\":[]}\n",
    "    dire_dict = {\"α=0.88\":[], \"α=0.9\":[]}\n",
    "    count_list = []\n",
    "\n",
    "    for line_now in corpus_now:\n",
    "     \n",
    "        sim_dot88 = 0\n",
    "        dot88 =True\n",
    "        defu_dot88 = 'null'\n",
    "        dir_dot88 = 'null'        \n",
    "        sim_dot9 = 0\n",
    "        dot9 =True\n",
    "        defu_dot9 = 'null'\n",
    "        dir_dot9 = 'null'        \n",
    "\n",
    "        for line_before in corpus_before:\n",
    "            common_elements = list(set(line_before[1][0].split(' ')).intersection(line_now[1][0].split(' ')))\n",
    "            #print(line_now[-1]+' '+line_before[-1])\n",
    "            if common_elements:\n",
    "                temp_sim_dot = simlirity_for_phrases(line_before[2:-1], line_now[2:-1])\n",
    "             \n",
    "                if dot88:\n",
    "                    if temp_sim_dot > sim_dot88:                            \n",
    "                        sim_dot88 = temp_sim_dot\n",
    "                        defu_dot88 = line_before[-1]+' '+line_now[-1]\n",
    "                        dir_dot88 = line_before[0][0]+','+line_now[0][0]\n",
    "                        if sim_dot88 >= 0.88:\n",
    "                            dot88 = False\n",
    "                if dot9:\n",
    "                    if temp_sim_dot > sim_dot9:                            \n",
    "                        sim_dot9 = temp_sim_dot\n",
    "                        defu_dot9 = line_before[-1]+' '+line_now[-1]\n",
    "                        dir_dot9 = line_before[0][0]+','+line_now[0][0]\n",
    "                        if sim_dot9 >= 0.9:\n",
    "                            dot9 = False                     \n",
    "                            break\n",
    "        \n",
    "        if dot88 or dot9:\n",
    "            for line_this in count_list:\n",
    "                common_this_elements = list(set(line_this[1][0].split(' ')).intersection(line_now[1][0].split(' ')))\n",
    "                if common_this_elements:\n",
    "                    temp_this_sim_dot = simlirity_for_phrases(line_this[2:-1], line_now[2:-1])\n",
    "                    if dot88:\n",
    "                        if temp_this_sim_dot > sim_dot88:\n",
    "                            sim_dot88 = temp_this_sim_dot\n",
    "                            defu_dot88 = line_this[-1]+' '+line_now[-1]\n",
    "                            dir_dot88 = line_this[0][0]+','+line_now[0][0]\n",
    "                            if sim_dot88 >= 0.88:\n",
    "                                dot88 = False \n",
    "\n",
    "                    if dot9:\n",
    "                        if temp_this_sim_dot > sim_dot9:                            \n",
    "                            sim_dot9 = temp_this_sim_dot\n",
    "                            defu_dot9 = line_this[-1]+' '+line_now[-1]\n",
    "                            dir_dot9 = line_this[0][0]+','+line_now[0][0]\n",
    "                            if sim_dot9 >= 0.9:\n",
    "                                dot9 = False                     \n",
    "                                break\n",
    "                                \n",
    "        if sim_dot88 < 0.88:\n",
    "            count_list.append(tuple(line_now))\n",
    "            inno_dict[\"α=0.88\"].append(line_now)\n",
    "        \n",
    "        sim_dict[\"α=0.88\"].append(sim_dot88)\n",
    "        defussion_dict[\"α=0.88\"].append(defu_dot88)\n",
    "        dire_dict[\"α=0.88\"].append(dir_dot88)\n",
    "        \n",
    "        if sim_dot9 < 0.9:\n",
    "            inno_dict[\"α=0.9\"].append(line_now)\n",
    "        sim_dict[\"α=0.9\"].append(sim_dot9)\n",
    "        defussion_dict[\"α=0.9\"].append(defu_dot9)\n",
    "        dire_dict[\"α=0.9\"].append(dir_dot9)\n",
    "        \n",
    "        \n",
    "        \n",
    "    return sim_dict,defussion_dict, dire_dict, inno_dict, count_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4be12820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['安徽省', '山西省', '福建省', '上海市', '陕西省', '广西壮族自治区', '宁夏回族自治区', '重庆市', '浙江省', '贵州省', '湖北省', '湖南省', '青海省', '黑龙江省', '河北省', '河南省', '江西省', '天津市', '国务院', '辽宁省', '江苏省', '新疆维吾尔自治区', '山东省', '甘肃省', '四川省', '海南省', '内蒙古自治区', '北京市', '云南省', '西藏自治区', '吉林省', '广东省']\n"
     ]
    }
   ],
   "source": [
    "province_list = os.listdir('../data/pre_processed')\n",
    "#province_list.remove('.DS_Store')\n",
    "#file_name_list.remove('国务院')\n",
    "print(province_list)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b1491a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "安徽省 1998\n",
      "709\n",
      "安徽省 1999\n",
      "1301\n",
      "563\n",
      "山西省 1999\n",
      "1092\n",
      "山西省 2000\n",
      "1525\n",
      "山西省 2001\n",
      "2012\n",
      "山西省 2002\n",
      "2476\n",
      "山西省 2003\n",
      "2898\n",
      "山西省 2004\n",
      "3445\n",
      "山西省 2005\n",
      "3839\n",
      "山西省 2006\n",
      "4317\n",
      "山西省 2007\n",
      "4754\n",
      "山西省 2008\n",
      "5216\n",
      "山西省 2009\n",
      "5703\n",
      "山西省 2010\n",
      "6034\n",
      "山西省 2011\n",
      "6372\n",
      "山西省 2012\n",
      "6663\n",
      "山西省 2013\n",
      "6979\n",
      "山西省 2014\n",
      "7348\n",
      "山西省 2015\n",
      "7713\n",
      "山西省 2016\n",
      "8135\n",
      "山西省 2017\n",
      "8858\n",
      "山西省 2018\n",
      "9506\n",
      "山西省 2019\n",
      "10070\n",
      "山西省 2020\n",
      "10629\n",
      "山西省 2021\n",
      "11138\n",
      "山西省 2022\n",
      "11647\n",
      "山西省 2023\n",
      "12319\n",
      "山西省 2024\n",
      "12819\n"
     ]
    }
   ],
   "source": [
    "#1119，该这里了\n",
    "for province in province_list[0:2]:\n",
    "    first_time = True\n",
    "    denominator_selfCorpus = []\n",
    "    #for year in time_list[1:]:\n",
    "    for time_year in range(1998,2025):  \n",
    "        save_path = \"../data/sig_reports/\"+province+str(time_year)+\"年政府工作报告.txt\"\n",
    "        if os.path.exists(save_path):\n",
    "            selfCorpus, num =read_phrases(save_path)\n",
    "            #if first_time:\n",
    "             #   first_time = False\n",
    "            #    denominator_selfCorpus.extend(selfCorpus)\n",
    "             #   continue\n",
    "            print(province, time_year)\n",
    "            simi_list,def_list, dir_list, inno_list,count_list = similirity_for_corpus(denominator_selfCorpus, selfCorpus)\n",
    "            denominator_selfCorpus.extend(count_list)\n",
    "            print(len(denominator_selfCorpus))\n",
    "            for alpha in [\"α=0.88\"]: \n",
    "                data_dict = {\"sim\": simi_list[alpha], \"sentences\": def_list[alpha], \"from\":dir_list[alpha]}\n",
    "                df = pd.DataFrame(data_dict)\n",
    "                excel_path = \"../data/adoption_judge/\"+province+str(time_year)+\"年政府工作报告_self_similarity.xlsx\"\n",
    "                df.to_excel(excel_path,index=False)\n",
    "\n",
    "                with open(file =\"../data/adoptions/\"+province+str(time_year)+\"年政府工作报告_self_similarity.txt\", mode='w',encoding='utf - 8') as f_forward:\n",
    "                    f_forward.write('innovation_num:  '+str(len(inno_list[alpha]))+num+'\\n')\n",
    "                    for every_line in inno_list[alpha]:\n",
    "                        f_forward.write(every_line[0][0]+',')\n",
    "                        f_forward.write(every_line[1][0]+',')\n",
    "                        for index_mode1_phrase, mode1_phrase in enumerate(every_line[2]):\n",
    "                            for index_mode1_word, mode1_word in enumerate(mode1_phrase):\n",
    "                                f_forward.write(mode1_word)\n",
    "                            if index_mode1_phrase< len(every_line[2])-1:\n",
    "                                f_forward.write(';')\n",
    "                        f_forward.write(',')\n",
    "                        for index_mode2_phrase, mode2_phrase in enumerate(every_line[3]):\n",
    "                            for index_mode2_word, mode2_word in enumerate(mode2_phrase):\n",
    "                                f_forward.write(mode2_word)\n",
    "                            if index_mode2_phrase< len(every_line[3])-1:\n",
    "                                f_forward.write(';') \n",
    "                        f_forward.write('。')\n",
    "                        f_forward.write(every_line[4])\n",
    "                        f_forward.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bee94257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "强高技能人才队伍建设 努力培养和壮大党政人才队伍 1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_Annotation_auto = pd.read_excel('../中国地方政府工作报告数据库/地级市政府工作报告/preprocessed_data/df_Annotation.xlsx', index_col=0)\n",
    "for index, row in df_Annotation_auto.iterrows():\n",
    "    s1 = row['sentence1']\n",
    "    s2 = row['sentence2']\n",
    "    label = row['label']\n",
    "    break\n",
    "print(s1, s2, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5bb6e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**这一标注正确。**\n",
      "> **最终结论：这一标注错误**\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "\n",
    "#list1 = []\n",
    "client = OpenAI(\n",
    "    # 若没有配置环境变量，请用百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "    api_key=\"sk-244031ceb83a4722a1abe204942fb30b\", # 如何获取API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "df_Annotation_auto = pd.read_excel('../中国地方政府工作报告数据库/地级市政府工作报告/preprocessed_data/df_Annotation.xlsx', index_col=0)\n",
    "for index, row in df_Annotation_auto[:10].iterrows():\n",
    "    s1 = row['sentence1']\n",
    "    s2 = row['sentence2']\n",
    "    label = row['label']\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"qwen-plus-latest\", # 模型列表：https://help.aliyun.com/zh/model-studio/getting-started/models\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"name\": \"annotator\", \n",
    "             \"content\": '你是一个 <annotator>, 也是一位在中国政府政策研究方面的专家。 您需要基于 <user>提供的来自中国政府工作报告中的两个代表政策举措的句子和对应的判断，基于五项原则进行校对。判断的label是0代表sentence2相对于sentence1是非创新，1则是创新。原则1，这些句子往往没有主语，以由动词、修饰词和宾语组成；在创新性比较中宾语最为重要，对政策举措的含义有决定性影响，如果两个句子的宾语含义差异较大，或者后一句子的并列宾语明显多于前者且含义超出前一句子的宾语，那么后一句子肯定相对于前者是创新的；其次重要的是直接修饰宾语的修饰词，再次是修饰词的修饰词，依次递减。原则2，句子中的动词是无关的，其含义不影响创新性判断结果，例如sentence1:加强城市化建设; sentence2:重视城市化建设,则sentence2相对于sentence1不是创新，因为\"重视\"和\"加强\"对结果没有影响。原则3，如果前一个政策举措的含义只是后一个政策举措的子集，例如，sentence1:做好民生工作; sentence2:做好民生、防灾工作，这种情况下后一个句子（即sentence2）相对于前一个句子（即sentence1）是创新的。原则4，如果两个句子的内容和结构完全不同，没有直接的可比性，则默认后一句子相对于前一句子是创新的。原则5，如果后一个句子的字数明显少于前一个句子，且核心单词都被前一个句子包含，那么后一个句子相对前一个句子不是创新，例如entence1:加强农村实用人才队伍建设; sentence2:加强人才队伍建设,则sentence2相对于sentence1不是创新。'},\n",
    "            {'role': 'user', 'content': '请指出对sentence2相对于sentence1的错误判断。请注意，label为1则判断结果为创新，label为2则判断结果为非创新'},\n",
    "            {'role': 'user', 'content': '如果某条记录的`label`标注为1但实际上应该是0，或者`label`标注为0但实际上应该是1，这些都属于错误判断。'},\n",
    "            {\"role\": \"user\", \"content\": \"判断这一标注是否正确，并以最终结论输出“这一标注正确”或“这一标注错误”：sentence1:\"+s1+\"; sentence2:\"+s2+\"; lanel:\"+str(label)+\"。\"}\n",
    "            ]\n",
    "    )\n",
    "\n",
    "\n",
    "    print(completion.choices[0].message.content)\n",
    "    #if \"这一标注正确\" in completion.choices[0].message.content:\n",
    "    #    print('yes')\n",
    "    #else:\n",
    "    #    print('no')\n",
    "    #    print()\n",
    "#print(list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3737578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"file-fe-a6725d7996d3485ca10c1e17\",\"bytes\":2490580,\"created_at\":1748009193,\"filename\":\"df_Annotation_auto.xlsx\",\"object\":\"file\",\"purpose\":\"file-extract\",\"status\":\"processed\",\"expires_at\":null,\"status_details\":null}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-244031ceb83a4722a1abe204942fb30b\", # 如何获取API Key：https://help.aliyun.com/zh/model-studio/developer-reference/get-api-key\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope服务base_url\n",
    ")\n",
    "\n",
    "file_object = client.files.create(file=Path(\"../中国地方政府工作报告数据库/地级市政府工作报告/preprocessed_data/df_Annotation_auto.xlsx\"), purpose=\"file-extract\")\n",
    "print(file_object.model_dump_json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9280a7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'根据您提供的原则和示例，我将检查表格中的每一项，以确定是否存在错误的创新性判断。以下是详细的分析和错误判断的列表：\\n\\n### 错误判断示例：\\n\\n#### 1. **Sheet1**\\n| Index | Sentence1 | Sentence2 | Label | 评注 |\\n|-------|------------|------------|-------|------|\\n| 0     | 落实职务待遇与业务支出管理规定 | 出台省政府立法工作规定 | 1 | 正确标注。两个句子的主题完全不同，Sentence2涉及的是立法工作，而Sentence1涉及的是职务待遇和业务支出管理。 |\\n| 1     | 推进创新型广东建设大力推进自主创新 | 深化法治建设四级同创 | 1 | 错误标注。两个句子的主题不同，Sentence1强调的是创新和自主创新，而Sentence2强调的是法治建设。应标注为0。 |\\n| 2     | 支持商品房市场更好满足购房者的合理住房需求 | 视同仁满足不同所有制房地产企业的合理融资需求 | 1 | 错误标注。虽然都提到“满足合理需求”，但Sentence1侧重于购房者的需求，而Sentence2侧重于房地产企业的融资需求，属于不同层面的需求。应标注为0。 |\\n| 3     | 企一策推动省属国企战略性重组 | 链一策拓长补短 | 1 | 错误标注。两个句子的主题不同，Sentence1是关于国企重组，而Sentence2是关于供应链的策略。应标注为0。 |\\n| 4     | 高质量谋划实施第二批授权事项清单 | 梳理细化超500项高频民生诉求事项职责清单 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 5     | 集中供养孤儿基本生活最低养育标准从每人每月1883元提高到1949元 | 集中供养孤儿基本生活保障省定最低标准从每人每月2017元提高到2295元 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 6     | 其中食用农产品批发市场实现全覆盖快检 | 食用农产品快检超800万批次 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 7     | 企业退休人员基本养老金增长10% | 高速公路服务区充电基础设施总车位达到小型客车停车位总数的10% | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及退休人员养老金，而Sentence2涉及高速公路服务区的充电设施。应标注为0。 |\\n| 8     | 完善养老保险省级统筹 | 为全省超1700家已备案托育机构统一购买在托婴幼儿意外责任保险 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 9     | 推动保障群众基本住房需求 | 全面保障特殊困难老年人助餐服务需求 | 1 | 错误标注。两个句子的主题不同，Sentence1涉及的是住房需求，而Sentence2涉及的是老年人助餐服务需求。应标注为0。 |\\n| 10    | 落实国家新的三年棚户区改造攻坚计划 | 提前一年完成十四五规划确定的8.6万户改造计划 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 11    | 全省所有圩镇达到宜居圩镇标准 | 受援医院基本达到国家县级医院医疗服务能力推荐标准 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是圩镇建设，而Sentence2涉及的是医院医疗服务能力。应标注为0。 |\\n| 12    | 建立涉农资金统筹整合长效机制 | 建立227所县域高中与市域优质高中结对帮扶机制 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是涉农资金统筹，而Sentence2涉及的是高中结对帮扶。应标注为0。 |\\n| 13    | 扩大订单定向医学生招收规模至每年1400名 | 推动工院校招生规模稳定在19万人以 | 1 | 错误标注。两个句子的主题不同，Sentence1涉及的是医学生招收，而Sentence2涉及的是工院校招生规模。应标注为0。 |\\n| 14    | 开展农村社区一站式综合服务示范创建 | 开展新一轮双拥模范城（县）创建 | 1 | 错误标注。两个句子的主题不同，Sentence1涉及的是农村社区服务，而Sentence2涉及的是双拥模范城创建。应标注为0。 |\\n| 15    | 推进节约集约用地示范省 | 推进生育友好省 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是节约集约用地，而Sentence2涉及的是生育友好。应标注为0。 |\\n| 16    | 加强海洋环境监测 | 加强平台企业就业监管监测 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是海洋环境监测，而Sentence2涉及的是平台企业就业监管。应标注为0。 |\\n| 17    | 落实新农保及城镇居民养老保险制度全覆盖财政资金安排 | 推广技培生等制度 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是养老保险制度，而Sentence2涉及的是技培生制度。应标注为0。 |\\n| 18    | 增强人民体质 | 增强青少年体质 | 1 | 正确标注。Sentence2在Sentence1的基础上限定了特定群体，是创新。 |\\n| 19    | 做大做强主流舆论 | 巩固壮大奋进新时代的主流思想舆论 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 20    | 开展改善消费品供给专项行动 | 全面实施空气质量持续改善行动 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是消费品供给，而Sentence2涉及的是空气质量改善。应标注为0。 |\\n| 21    | 加强红树林及湿地公园保护 | 守护好红树林 | 1 | 错误标注。两个句子的主题不同，Sentence1涉及的是红树林及湿地公园保护，而Sentence2仅涉及红树林保护。应标注为0。 |\\n| 22    | 提升城市综合品质 | 提升海洋生态品质 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是城市综合品质，而Sentence2涉及的是海洋生态品质。应标注为0。 |\\n| 23    | 推进全国旅游综合改革示范区 | 推进琼州海峡一体化高质量发展示范区 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是旅游综合改革，而Sentence2涉及的是琼州海峡一体化发展。应标注为0。 |\\n| 24    | 发展海洋航运 | 推动内河航运与海洋运输贯通 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 25    | 推动保障群众基本住房需求 | 保障重大项目用海需求 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是住房需求，而Sentence2涉及的是用海需求。应标注为0。 |\\n| 26    | 加快制定全省海洋主体功能区规划 | 出台省海岸带及海洋空间规划 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 27    | 用好国际友城等外事资源 | 汇聚全球高端资源 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 28    | 鼓励跨国公司在粤设立总部型企业 | 支持广东企业设立海外销售公司 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是跨国公司设立总部，而Sentence2涉及的是广东企业设立海外销售公司。应标注为0。 |\\n| 29    | 增强国际影响力 | 扩大粤港澳大湾区全球招商大会影响力 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 30    | 打造消费节庆品牌 | 打造投资广东品牌 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是消费节庆品牌，而Sentence2涉及的是投资品牌。应标注为0。 |\\n| 31    | 推进政务服务事项标准化 | 推动政务服务标准化规范化便利化 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 32    | 企一策推动省属国企战略性重组 | 企一策推进原创技术策源地 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是国企重组，而Sentence2涉及的是原创技术策源地。应标注为0。 |\\n| 33    | 推广三网融合应用 | 推广数字人民币应用 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是三网融合应用，而Sentence2涉及的是数字人民币应用。应标注为0。 |\\n| 34    | 50%以上的资金用于支持发展养老服务业 | 鼓励保险资金投资养老产业 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 35    | 推进54个试点县普惠金融村村通 | 做强养老金融 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是普惠金融，而Sentence2涉及的是养老金融。应标注为0。 |\\n| 36    | 培育壮大天使投资人群体 | 培育高质量上市公司群体 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是天使投资人，而Sentence2涉及的是上市公司群体。应标注为0。 |\\n| 37    | 推进财政管理层级扁平化 | 减少供电层级 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是财政管理层级，而Sentence2涉及的是供电层级。应标注为0。 |\\n| 38    | 建立健全促进农民收入较快增长的长效机制 | 健全防范化解拖欠中小企业账款长效机制 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是农民收入增长机制，而Sentence2涉及的是中小企业账款防范机制。应标注为0。 |\\n| 39    | 建立健全依法及时处理群众反映问题的长效机制 | 优化经营主体反映问题快速响应处理机制 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 40    | 加快建立与基本公共服务均等化相适应的公共财政体系 | 推动各类公共资源交易纳入统一平台体系 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是公共财政体系，而Sentence2涉及的是公共资源交易平台。应标注为0。 |\\n| 41    | 加快城镇污水处理设施及配套管网 | 建设一批地下管网 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是污水处理设施及配套管网，而Sentence2涉及的是地下管网建设。应标注为0。 |\\n| 42    | 构建大湾区城际快速交通网络 | 规划建设雷州半岛输水储水网络 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是交通网络，而Sentence2涉及的是输水储水网络。应标注为0。 |\\n| 43    | 加快珠三角绿色生态水网 | 推进全省水网 | 1 | 错误标注。两个句子的主题不同，Sentence1涉及的是珠三角绿色生态水网，而Sentence2涉及的是全省水网。应标注为0。 |\\n| 44    | 做大做强体育产业 | 推动产业与科技互促双强 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是体育产业，而Sentence2涉及的是产业与科技互促。应标注为0。 |\\n| 45    | 推动高水平高质量普及高中阶段教育 | 高质量推动科学普及 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是高中阶段教育，而Sentence2涉及的是科学普及。应标注为0。 |\\n| 46    | 促进一流高职院校 | 加强高水平职业院校 | 1 | 错误标注。两个句子的主题完全不同，Sentence1涉及的是一流高职院校，而Sentence2涉及的是高水平职业院校。应标注为0。 |\\n| 47    | 探索关键核心技术攻关新型举国体制广东路径 | 探索关键核心技术攻关新型举国体制的广东实践 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 48    | 制定完善装备制造业发展规划及实施方案 | 制定发布新一批制造业标准体系规划与路线图 | 1 | 正确标注。Sentence2在Sentence1的基础上增加了具体细节，是创新。 |\\n| 49    | 制定实施促进服务业投资发展的政策措施 | 出台促进生产性服务业发展的政策文件 | 1 | 错误标注。两个句子的主题不同，Sentence1涉及的是服务业投资发展，而Sentence2涉及的是生产性服务业发展。应标注为0。 |\\n| 50    | 实施高新技术企业树标提质行动 |  |  | Sentence2为空，无法判断。 |\\n\\n#### 2. **其他表单**\\n由于篇幅较长，这里只展示了部分错误判断，您可以继续按照上述方法检查其他表单中的记录。\\n\\n### 总结：\\n根据上述分析，表格中存在多个错误标注的记录，具体错误标注的原因主要是两个句子的主题不同或Sentence2在Sentence1的基础上增加了具体细节，但并未扩展或改变了核心主题。建议对标注为1但实际上应该是0的记录进行修正，反之亦然。'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=\"sk-244031ceb83a4722a1abe204942fb30b\",  # 如果您没有配置环境变量，请在此处替换您的API-KEY\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",  # 填写DashScope服务base_url\n",
    ")\n",
    "# 初始化messages列表\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen-long\",\n",
    "    messages=[\n",
    "        {'role': 'system', \"content\": '你是一个 <judger>, 也是一位在中国政策研究方面的专家。你需要基于 <user>提供的excel文件列出其中的错误判断。文件中是标注过的正确分类，其中label是0代表sentence2相对于sentence1是非创新，1则是创新。原则1，这些句子往往没有主语，以由动词、修饰词和宾语组成；在创新性比较中宾语最为重要，对政策举措的含义有决定性影响，如果两个句子的宾语含义差异较大，或者后一句子的并列宾语明显多于前者且含义超出前一句子的宾语，那么后一句子肯定相对于前者是创新的；其次重要的是直接修饰宾语的修饰词，再次是修饰词的修饰词，依次递减。原则2，句子中的动词是无关的，其含义不影响创新性判断结果，例如sentence1:加强城市化建设; sentence2:重视城市化建设,则sentence2相对于sentence1不是创新，因为\"重视\"和\"加强\"对结果没有影响。原则3，如果前一个政策举措的含义只是后一个政策举措的子集，例如，sentence1:做好民生工作; sentence2:做好民生、防灾工作，这种情况下后一个句子（即sentence2）相对于前一个句子（即sentence1）是创新的。原则4，如果两个句子的内容和结构完全不同，没有直接的可比性，则默认后一句子相对于前一句子是创新的。如果后一个句子的字数明显少于前一个句子，且核心单词都被前一个句子包含，那么后一个句子相对前一个句子不是创新，例如entence1:加强农村实用人才队伍建设; sentence2:加强人才队伍建设,则sentence2相对于sentence1不是创新。'},\n",
    "        # 请将 'file-fe-xxx'替换为您实际对话场景所使用的 file-id。\n",
    "        {'role': 'system', 'content': 'fileid://file-fe-a6725d7996d3485ca10c1e17'},\n",
    "        {'role': 'user', 'content': '请指出对sentence2相对于sentence1的错误判断。请注意，label为1则判断结果为创新，label为2则判断结果为非创新'},\n",
    "        {'role': 'user', 'content': '如果某条记录的`label`标注为1但实际上应该是0，或者`label`标注为0但实际上应该是1，这些都属于错误判断。'},\n",
    "        {'role': 'user', 'content': '例如，Sentence1: 落实职务待遇与业务支出管理规定\\n  - Sentence2: 出台省政府立法工作规定 - label: 1, 这个就是正确标注，因为两个句子的主题完全不同，Sentence2涉及的是立法工作，而Sentence1涉及的是职务待遇和业务支出管理'}\n",
    "    ],\n",
    "    stream=True,\n",
    "    stream_options={\"include_usage\": True}\n",
    ")\n",
    "\n",
    "full_content = \"\"\n",
    "for chunk in completion:\n",
    "    if chunk.choices and chunk.choices[0].delta.content:\n",
    "        # 拼接输出内容\n",
    "        full_content += chunk.choices[0].delta.content\n",
    "        #print(chunk.model_dump())\n",
    "\n",
    "print({full_content})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00106168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
