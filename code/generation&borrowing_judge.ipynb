{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c8f6604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "355990it [00:16, 21161.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import random\n",
    "import jieba\n",
    "import hanlp\n",
    "import time\n",
    "import pandas as pd\n",
    "#import torch\n",
    "import openpyxl\n",
    "#from torch import cosine_similarity\n",
    "\n",
    "#from text2vec import SentenceModel, cos_sim, semantic_search\n",
    "from datetime import datetime\n",
    "#从自写工具包中导入：修饰词判定、修饰词查找\n",
    "from utils import modifier_judge, modifier_search\n",
    "\n",
    "#device = torch.device('cuda')\n",
    "#use_gpu = torch.cuda.is_available()\n",
    "\n",
    "def read_vectors(path, topn):  # read top n word vectors, i.e. top is 10000\n",
    "    lines_num, dim = 0, 0\n",
    "    vectors = {}\n",
    "    iw = []\n",
    "    wi = {}\n",
    "    #num_file = sum([1 for i in open(path, encoding='utf-8', errors='ignore')])  \n",
    "    #print(num_file)\n",
    "    with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "        first_line = True\n",
    "        for line in tqdm(f):\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                dim = int(line.rstrip().split()[1])\n",
    "                continue\n",
    "            lines_num += 1\n",
    "            tokens = line.rstrip().split(' ')\n",
    "            vectors[tokens[0]] = np.expand_dims(np.asarray([float(x) for x in tokens[1:]]),axis=0)\n",
    "            #vectors[tokens[0]] = torch.tensor([float(x) for x in tokens[1:]], device=device).unsqueeze(0)\n",
    "            iw.append(tokens[0])\n",
    "            if topn != 0 and lines_num >= topn:\n",
    "                break\n",
    "    for i, w in enumerate(iw):\n",
    "        wi[w] = i\n",
    "    return vectors, iw, wi, dim\n",
    "#Use your pre-trained file and put it in data/pre_trained_vactors\n",
    "vectors_path = \"../data/pre_trained_vectors/sgns.renmin.bigram\"\n",
    "topn = 0\n",
    "vectors, iw, wi, dim = read_vectors(vectors_path, topn) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e94ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000002\n",
      "-0.5\n"
     ]
    }
   ],
   "source": [
    "def cos_sim(vec1, vec2):\n",
    "    if vec1.all() == 0 or vec2.all() == 0:\n",
    "        return np.zeros([1])[0]\n",
    "    else:\n",
    "        return (vec1.dot(vec2[0]) / (np.linalg.norm(vec1[0]) * np.linalg.norm(vec2[0])))[0]\n",
    "\n",
    "\n",
    "\n",
    "def word2vec(word1):\n",
    "    if word1 in vectors:\n",
    "        return vectors[word1]\n",
    "    else:\n",
    "        #print('not find')\n",
    "        first_line = True\n",
    "        for character in word1:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                if character in vectors:\n",
    "                    vector = vectors[character]\n",
    "                else: \n",
    "                    vector = np.zeros([1, 300])\n",
    "                continue\n",
    "            if character in vectors:\n",
    "                vector = np.append(vector,vectors[character], axis = 0)\n",
    "        return np.mean(vector, axis = 0).reshape(1,300)\n",
    "\n",
    "def words2vec(word_list):\n",
    "    first_line = True\n",
    "    for word in word_list:\n",
    "        if first_line:\n",
    "            first_line = False\n",
    "            vector = word2vec(word)\n",
    "            continue\n",
    "        vector =  np.append(vector,word2vec(word), axis = 0)\n",
    "    return np.mean(vector, axis = 0).reshape(1,300)\n",
    "\n",
    "\n",
    "def save_file(path, the_list, num_name, t_num):\n",
    "    \n",
    "    with open(file =path, mode='w',encoding='utf - 8') as f_bor:\n",
    "        f_bor.write(num_name + ': '+str(len(the_list))+t_num)\n",
    "        for every_line_bor in the_list:\n",
    "            f_bor.write(every_line_bor[0][0]+',')\n",
    "            f_bor.write(every_line_bor[1][0]+',')\n",
    "            for index_mode1_phrase_bor, mode1_phrase_bor in enumerate(every_line_bor[2]):\n",
    "                for mode1_word_bor in mode1_phrase_bor:\n",
    "                    f_bor.write(mode1_word_bor)\n",
    "                if index_mode1_phrase_bor< len(every_line_bor[2])-1:\n",
    "                    f_bor.write(';')\n",
    "            f_bor.write(',')\n",
    "            for index_mode2_phrase_bor, mode2_phrase_bor in enumerate(every_line_bor[3]):\n",
    "                for mode2_word_bor in mode2_phrase_bor:\n",
    "                    f_bor.write(mode2_word_bor)\n",
    "                if index_mode2_phrase_bor< len(every_line_bor[3])-1:\n",
    "                    f_bor.write(';') \n",
    "            f_bor.write('。')\n",
    "            f_bor.write(every_line_bor[4])\n",
    "            f_bor.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "010df6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_phrases(path):\n",
    "    with open (path, 'r',  encoding='utf-8') as phrases_read:\n",
    "        phrases = []\n",
    "        first_line = True\n",
    "        for line in phrases_read:\n",
    "            if first_line:\n",
    "                first_line = False\n",
    "                line_number = line\n",
    "                continue\n",
    "            if line != '\\n':\n",
    "                cut = line.replace('\\n', '').split('。')\n",
    "                phrase = cut[0].split(',')\n",
    "                sentence = cut[1]\n",
    "                phrase = [tuple(x.split(';')) for x in phrase]\n",
    "                phrase.append(sentence)\n",
    "                phrases.append(tuple(phrase))\n",
    "    return phrases, line_number\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#老版：0.1*sim_v+0.4*sim_key+0.5*sim_modify\n",
    "def simlirity_for_phase_mode1(phase_before, phase_now):\n",
    "    #print(phase_before, phase_now)\n",
    "    sim_v = cos_sim(word2vec(phase_before[0]),word2vec(phase_now[0]))\n",
    "    sim_key = cos_sim(word2vec(phase_before[1]),word2vec(phase_now[1]))\n",
    "    #print(sim_key, sim_v)\n",
    "    sim_modify = np.zeros([1])[0]\n",
    "    if len(phase_now)>2:\n",
    "        if len(phase_before)>2:\n",
    "            for word_now in phase_now[2:]:\n",
    "                sim_modify_tem = np.zeros([1])[0]\n",
    "                for word_before in phase_before[2:]:\n",
    "                    tem_sim = cos_sim(word2vec(word_before),word2vec(word_now))\n",
    "                    if tem_sim > sim_modify_tem:\n",
    "                        sim_modify_tem = tem_sim\n",
    "                sim_modify = sim_modify + sim_modify_tem\n",
    "            sim_modify = sim_modify/len(phase_now[2:])\n",
    "            return 0.1*sim_v+0.5*sim_key+0.4*sim_modify\n",
    "        else:\n",
    "            return 0.1*sim_v+0.6*sim_key\n",
    "    else:\n",
    "        return 0.14*sim_v+0.86*sim_key\n",
    "\n",
    "def simlirity_for_phase_mode2(phase_before, phase_now):\n",
    "    sim_key = cos_sim(word2vec(phase_before[0]),word2vec(phase_now[0]))\n",
    "    sim_modify = np.zeros([1])[0]\n",
    "    if len(phase_now)>1:\n",
    "        if len(phase_before)>1:\n",
    "            sim_modify = cos_sim(words2vec(phase_before[1:]),words2vec(phase_now[1:]))\n",
    "            return 0.4*sim_key+0.6*sim_modify\n",
    "        else:\n",
    "            return 0.4*sim_key\n",
    "    else:\n",
    "        return sim_key\n",
    "\n",
    "def simlirity_for_phrases(phrases_before, phrases_now):\n",
    "    sum1 = np.zeros([1])[0]\n",
    "    sum2 = np.zeros([1])[0]\n",
    "    count1 = len(phrases_now[0])\n",
    "    count2 = len(phrases_now[1])\n",
    "    another_mode = False\n",
    "    if len(phrases_now[1])> len(phrases_before[1]):\n",
    "        another_mode = True\n",
    "    temp_phrases_now = [list(i_now) for i_now in phrases_now]\n",
    "    temp_phrases_before = [list(i_before) for i_before in phrases_before]\n",
    "    #print(phrases_before, phrases_now)\n",
    "    #print(temp_phrases_before, temp_phrases_now)\n",
    "    if phrases_now[0][0] and phrases_now[1][0]:\n",
    "        the_type = True\n",
    "    else:\n",
    "        the_type = False\n",
    "    #现在的无mode2，过去无mode2\n",
    "    #print(phrases_before, phrases_now)\n",
    "    #print(phrases_before[1][0], phrases_now[1])\n",
    "    while temp_phrases_now[0][0] and temp_phrases_before[0][0]:\n",
    "        sim1_2 = np.zeros([1])[0]\n",
    "        for index1_1, phrase_now in enumerate(temp_phrases_now[0]):            \n",
    "            for index1_2, phrase_before in enumerate(temp_phrases_before[0]):\n",
    "                sim_temp1 = simlirity_for_phase_mode1(phrase_before.split(' '), phrase_now.split(' '))\n",
    "                #print(sim_temp1)\n",
    "                if sim_temp1 > sim1_2:\n",
    "                    pos1_1 = phrase_now\n",
    "                    pos1_2 = phrase_before\n",
    "                    sim1_2 = sim_temp1\n",
    "        sum1 = sum1 + sim1_2\n",
    "        if len(temp_phrases_now[0]) == 1 or len(temp_phrases_before[0]) == 1:\n",
    "            break\n",
    "        temp_phrases_now[0].remove(pos1_1)\n",
    "        temp_phrases_before[0].remove(pos1_2)\n",
    "    \n",
    "    while temp_phrases_now[1][0] and temp_phrases_before[1][0]:\n",
    "        sim2_2 = np.zeros([1])[0]-0.5\n",
    "        pos2_1 = ''\n",
    "        pos2_2 = ''\n",
    "        for index2_1, phrase_now2 in enumerate(temp_phrases_now[1]):\n",
    "            for index2_2, phrase_before2 in enumerate(temp_phrases_before[1]):\n",
    "                sim_temp2 = simlirity_for_phase_mode2(phrase_before2.split(' '), phrase_now2.split(' '))\n",
    "                if sim_temp2 > sim2_2:\n",
    "                    pos2_1 = phrase_now2\n",
    "                    pos2_2 = phrase_before2\n",
    "                    sim2_2 = sim_temp2\n",
    "        sum2 = sum2 + sim2_2\n",
    "        if len(temp_phrases_now[1]) == 1 or len(temp_phrases_before[1]) == 1:\n",
    "            if another_mode and pos2_1 in temp_phrases_now[1]:\n",
    "                temp_phrases_now[1].remove(pos2_1)\n",
    "            break\n",
    "        try:\n",
    "            temp_phrases_now[1].remove(pos2_1)\n",
    "            temp_phrases_before[1].remove(pos2_2)\n",
    "        except:\n",
    "            print(phrases_before, phrases_now)\n",
    "            print(temp_phrases_before, temp_phrases_now)\n",
    "            print(pos2_1, pos2_2)\n",
    "            print('mode2')\n",
    "            print(sim_temp2)\n",
    "            print(sim2_2)\n",
    "            break\n",
    "            \n",
    "    if another_mode:\n",
    "        while temp_phrases_now[1][0] and temp_phrases_before[0][0]:\n",
    "            sim2_3 = np.zeros([1])[0]\n",
    "            for index23_1, phrase_now23 in enumerate(temp_phrases_now[1]):\n",
    "                for index23_2, phrase_before23 in enumerate(temp_phrases_before[0]):\n",
    "                    sim_temp23 = simlirity_for_phase_mode2(phrase_before23.split(' ')[1:], phrase_now23.split(' '))\n",
    "                    if sim_temp23 > sim2_3:\n",
    "                        pos23_1 = phrase_now23\n",
    "                        pos23_2 = phrase_before23\n",
    "                        sim2_3 = sim_temp23\n",
    "            sum2 = sum2 + sim2_3\n",
    "            if len(temp_phrases_now[1]) == 1 or len(temp_phrases_before[0]) == 1:\n",
    "                break\n",
    "            try:\n",
    "                temp_phrases_now[1].remove(pos23_1)\n",
    "            except:\n",
    "                print(phrases_before, phrases_now)\n",
    "                print(temp_phrases_before, temp_phrases_now, pos23_1, pos23_2)\n",
    "                break\n",
    "    \n",
    "    if count1 > 0 :\n",
    "        sim1 = sum1/count1\n",
    "    if count2 > 0 :\n",
    "        sim2 = sum2/count2\n",
    "    \n",
    "    if the_type:\n",
    "        return 0.77*sim1 + 0.23*sim2\n",
    "    else:\n",
    "        return sim1 + sim2\n",
    "        \n",
    "        \n",
    "def similirity_for_corpus(corpus_before_borrowed, corpus_before_unborrowed, corpus_now):\n",
    "    generation_candidates=[]\n",
    "    borrowing_candidates=[]\n",
    "    sim_dict=[]\n",
    "    defussion_dict = []\n",
    "    dire_dict = []\n",
    "    judge_dict = []\n",
    "    \n",
    "    count_list = []\n",
    "    kind = ''\n",
    "\n",
    "    for line_now in corpus_now:\n",
    "     \n",
    "        sim_dot88 = 0\n",
    "        dot88 =True\n",
    "        defu_dot88 = 'null'\n",
    "        dir_dot88 = 'null'           \n",
    "\n",
    "        for line_before_borrowed in corpus_before_borrowed:\n",
    "            common_elements_borrowed = list(set(line_before_borrowed[1][0].split(' ')).intersection(line_now[1][0].split(' ')))\n",
    "            #print(line_now[-1]+' '+line_before[-1])\n",
    "            if common_elements_borrowed:\n",
    "                temp_sim_dot_borrowed = simlirity_for_phrases(line_before_borrowed[2:-1], line_now[2:-1])\n",
    "             \n",
    "                if dot88:\n",
    "                    if temp_sim_dot_borrowed > sim_dot88:                            \n",
    "                        sim_dot88 = temp_sim_dot_borrowed\n",
    "                        defu_dot88 = line_before_borrowed[-1]+' '+line_now[-1]\n",
    "                        dir_dot88 = line_before_borrowed[0][0]+','+line_now[0][0]\n",
    "                        if sim_dot88 >= 0.88:  \n",
    "                            dot88 = False\n",
    "                            break\n",
    "                            \n",
    "        if dot88:\n",
    "            for line_before_unborrowed in corpus_before_unborrowed:\n",
    "                common_elements_unborrowed = list(set(line_before_unborrowed[1][0].split(' ')).intersection(line_now[1][0].split(' ')))\n",
    "                #print(line_now[-1]+' '+line_before[-1])\n",
    "                if common_elements_unborrowed:\n",
    "                    temp_sim_dot_unborrowed = simlirity_for_phrases(line_before_unborrowed[2:-1], line_now[2:-1])\n",
    "                    if temp_sim_dot_unborrowed > sim_dot88:                            \n",
    "                        sim_dot88 = temp_sim_dot_unborrowed\n",
    "                        defu_dot88 = line_before_unborrowed[-1]+' '+line_now[-1]\n",
    "                        dir_dot88 = line_before_unborrowed[0][0]+','+line_now[0][0]\n",
    "                        if sim_dot88 >= 0.88:\n",
    "                            corpus_before_borrowed.insert(0, tuple(line_before_unborrowed))\n",
    "                            corpus_before_unborrowed.remove(line_before_unborrowed)\n",
    "                            dot88 = False\n",
    "                            break\n",
    "                                \n",
    "        if sim_dot88 < 0.88:\n",
    "            count_list.append(tuple(line_now))\n",
    "            generation_candidates.append(line_now)\n",
    "            kind = 'generation'\n",
    "            \n",
    "        if sim_dot88 >= 0.88:\n",
    "            kind = 'borrowing'\n",
    "            borrowing_candidates.append(line_now)\n",
    "            \n",
    "        sim_dict.append(sim_dot88)\n",
    "        defussion_dict.append(defu_dot88)\n",
    "        dire_dict.append(dir_dot88)\n",
    "        judge_dict.append(kind)\n",
    "                    \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    return sim_dict, defussion_dict, dire_dict, judge_dict, generation_candidates, borrowing_candidates, count_list, corpus_before_borrowed, corpus_before_unborrowed\n",
    "\n",
    "\n",
    " \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb66ab0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['安徽省', '山西省', '福建省', '上海市', '陕西省', '广西壮族自治区', '宁夏回族自治区', '重庆市', '浙江省', '贵州省', '湖北省', '湖南省', '青海省', '黑龙江省', '河北省', '河南省', '江西省', '天津市', '国务院', '辽宁省', '江苏省', '新疆维吾尔自治区', '山东省', '甘肃省', '四川省', '海南省', '内蒙古自治区', '北京市', '云南省', '西藏自治区', '吉林省', '广东省']\n"
     ]
    }
   ],
   "source": [
    "province_list =['安徽省', '山西省', '福建省', '上海市', '陕西省', '广西壮族自治区', '宁夏回族自治区', '重庆市', '浙江省', '贵州省', '湖北省', '湖南省', '青海省', '黑龙江省', '河北省', '河南省', '江西省', '天津市', '国务院', '辽宁省', '江苏省', '新疆维吾尔自治区', '山东省', '甘肃省', '四川省', '海南省', '内蒙古自治区', '北京市', '云南省', '西藏自治区', '吉林省', '广东省']\n",
    "#province_list.remove('.DS_Store')\n",
    "#file_name_list.remove('国务院')\n",
    "print(province_list)\n",
    "\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21bd9f42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "四川省 2024\n",
      "40457 202838\n",
      "海南省 2024\n",
      "40488 202807\n",
      "内蒙古自治区 2024\n",
      "40536 202759\n",
      "北京市 2024\n",
      "40586 202709\n",
      "云南省 2024\n",
      "40621 202674\n",
      "西藏自治区 2024\n",
      "40639 202656\n",
      "吉林省 2024\n",
      "40692 202603\n",
      "广东省 2024\n",
      "40771 202524\n"
     ]
    }
   ],
   "source": [
    "first_time = True\n",
    "#calculate year by year, from 1998 to 2023\n",
    "time_year = 2024\n",
    "alpha =\"α=0.88\"\n",
    "\n",
    "history_path = \"../results/history_innovation_base/\"\n",
    "history_temp_path = \"../results/history_temp/\"\n",
    "\n",
    "denominator_allCorpus_borrowed = []\n",
    "denominator_allCorpus_unborrowed = []\n",
    "if os.path.exists(history_path+'borrowed_till'+str(time_year-1)+'.txt'):\n",
    "    denominator_allCorpus_borrowed, num_borrowed =read_phrases(history_path+'borrowed_till'+str(time_year-1)+'.txt')\n",
    "if os.path.exists(history_path+'unborrowed_till'+str(time_year-1)+'.txt'):\n",
    "    denominator_allCorpus_unborrowed, num_unborrowed =read_phrases(history_path+'unborrowed_till'+str(time_year-1)+'.txt')\n",
    "\n",
    "temp_corpus = []\n",
    "for province in province_list:\n",
    "    save_path = \"../results/adoptions/\"+province+str(time_year)+\"年政府工作报告_self_similarity.txt\"\n",
    "    if os.path.exists(save_path):\n",
    "        adoptionCorpus, num =read_phrases(save_path)\n",
    "        print(province, time_year)\n",
    "        simi_list,def_list, dir_list, judge_list, generation_list, borrowing_list, count_list, denominator_allCorpus_borrowed, denominator_allCorpus_unborrowed = similirity_for_corpus(denominator_allCorpus_borrowed, denominator_allCorpus_unborrowed, adoptionCorpus)\n",
    "        temp_corpus = temp_corpus + count_list\n",
    "        data_dict = {\"sim\": simi_list, \"sentences\": def_list, \"from\":dir_list, \"kinds\":judge_list}\n",
    "        df = pd.DataFrame(data_dict)\n",
    "        excel_path = \"../results/generation&borrowing_judge/\"+province+str(time_year)+\"年政府工作报告_generation&borrowing_judge.xlsx\"\n",
    "        df.to_excel(excel_path,index=False)\n",
    "        print(len(denominator_allCorpus_borrowed), len(denominator_allCorpus_unborrowed))\n",
    "        \n",
    "        save_file(\"../results/generations/\"+province+str(time_year)+\"generations.txt\", generation_list, 'generation_num', num)\n",
    "        save_file(\"../results/borrowings/\"+province+str(time_year)+\"borrowings.txt\", borrowing_list, 'borrowing_num', num)\n",
    "\n",
    "denominator_allCorpus_unborrowed = denominator_allCorpus_unborrowed + temp_corpus\n",
    "save_file(history_temp_path+'borrowed_till'+str(time_year)+'d.txt', denominator_allCorpus_borrowed, 'borrowed_num', '')\n",
    "save_file(history_temp_path+'unborrowed_till'+str(time_year)+'d.txt', denominator_allCorpus_unborrowed, 'unborrowed_num', '')                    \n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d947e0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29584 184926\n",
      "8580 53219\n",
      "8580 51566\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_duplicates(lst):\n",
    "    return list(set(lst))\n",
    "\n",
    "organizing_allCorpus_borrowed = []\n",
    "organizing_allCorpus_unborrowed = []\n",
    "de_list = []\n",
    "\n",
    "for x in ['a', 'b', 'c', 'd']:\n",
    "    organizing_allCorpus_borrowed = organizing_allCorpus_borrowed +read_phrases(history_temp_path+'borrowed_till'+str(time_year)+x+'.txt')[0]\n",
    "    organizing_allCorpus_unborrowed = organizing_allCorpus_unborrowed +read_phrases(history_temp_path+'unborrowed_till'+str(time_year)+x+'.txt')[0]\n",
    "\n",
    "print(len(organizing_allCorpus_borrowed), len(organizing_allCorpus_unborrowed))\n",
    "\n",
    "organizing_allCorpus_borrowed = remove_duplicates(organizing_allCorpus_borrowed)\n",
    "organizing_allCorpus_unborrowed = remove_duplicates(organizing_allCorpus_unborrowed)\n",
    "\n",
    "print(len(organizing_allCorpus_borrowed), len(organizing_allCorpus_unborrowed))\n",
    "\n",
    "for borrowed_one in organizing_allCorpus_borrowed:\n",
    "    for unborrowed_one in organizing_allCorpus_unborrowed:\n",
    "        if borrowed_one == unborrowed_one:\n",
    "            de_list.append(borrowed_one)\n",
    "            continue\n",
    "            \n",
    "for de_one in de_list:\n",
    "    organizing_allCorpus_unborrowed.remove(de_one)\n",
    "\n",
    "print(len(organizing_allCorpus_borrowed), len(organizing_allCorpus_unborrowed))\n",
    "\n",
    "organizing_allCorpus_unborrowed.sort(key=lambda x: x[0][0].split(' ')[0][-4:])\n",
    "\n",
    "save_file(history_path+'borrowed_till'+str(time_year)+'.txt', organizing_allCorpus_borrowed, 'borrowed_num', '\\n')\n",
    "save_file(history_path+'unborrowed_till'+str(time_year)+'.txt', organizing_allCorpus_unborrowed, 'unborrowed_num', '\\n')                    \n",
    "#save_file(history_path+'unborrowed_till'+str(time_year)+'unremoved.txt', organizing_allCorpus_unborrowed, 'unborrowed_num', '\\n')                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc216e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "delis, num_unborrowed =read_phrases(history_path+'unborrowed_till2023unremoved.txt')\n",
    "# 在原地删除包含元素2的子列表\n",
    "for sublist in delis[:20000]:\n",
    "    if '2008' in sublist[0][0] :\n",
    "        #print(sublist[0][0].split(' ')[0][-4:], sublist[0][0].split(' ')[1])\n",
    "        delis.remove(sublist)\n",
    "        \n",
    "save_file(history_path+'unborrowed_till'+str(time_year)+'.txt', delis, 'unborrowed_num', '\\n')                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
